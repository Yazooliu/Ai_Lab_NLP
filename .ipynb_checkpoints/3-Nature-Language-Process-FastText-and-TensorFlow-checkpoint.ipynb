{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然语言处理之 文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow need run under:\n",
      " 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#  Copyright private in 2018 \n",
    "#  Modify Date: \n",
    "#       2018 - 9 - 19\n",
    "#  Purpose : \n",
    "#       Text Analysise  by fasttext/word2vec/Deep learning/LSTM\n",
    "# ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import sklearn \n",
    "import sys \n",
    "print (\"TensorFlow need run under:\\n\",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hierarchical softmax - 类别较多时，通过构建哈夫曼编码来技术softmax layer 计算 和之前的word2vec 的trick\n",
    "# N-gram - 之使用unigram 的话会丢掉word order 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当文本量大时，可以通过fasttext来学习\n",
    "# data Exampe: \n",
    "#__label__2,.......content ......\n",
    "#__label__3,......content......\n",
    "#__label__4,......content......\n",
    "\n",
    "# Data Category \n",
    "# 1. car 2.sports 3.entertainment 4. technology 5. military"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (一). 通过Facebook 工业界fasttext 模型根据输入的新闻内容预测该新闻所属的种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督学习 - 新闻分类/或者用于用户情感的褒贬分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 生成文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import random \n",
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sprots\n",
    "df_sports   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sports   = df_sports.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"./data/military_news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n"
     ]
    }
   ],
   "source": [
    "print (technology[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　伴随全新途观L的上市，上汽大众大众品牌刷新了SUV产品谱系，构建起包含途观丝绸之路版、全新途观L及Teramont途昂在内的SUV产品矩阵，覆盖多个SUV细分市场，为消费者提供更为丰富的购车选择与更优质的汽车生活，助力上汽大众赢得更亮眼的市场表现。\n"
     ]
    }
   ],
   "source": [
    "print car[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　基于优酷强势内容，霸屏客厅的阿里家庭娱乐春节期间发出花式大招，精心编排，巧妙聚合，“春晚热播，明星扎堆，心疼爸妈，今年绝不熬着看”。花式看春晚第一弹是“明星组团儿上春晚”，囊括沈腾、贾玲、曹云金、小沈阳、冯巩、郭德纲、宋小宝、岳云鹏、青岛大姨、赵本山、蔡明、潘长江、郭冬临、黄宏、巩汉林共15位春晚笑匠，集合每个笑星历年春晚或综艺作品，将其精彩演出片段集结成辑打包放出。第二弹是以专题的形式聚合2017年春晚语言/歌曲/魔术戏曲三大类型，横向打穿，爱看小品的老爸老妈，追星的迷妹迷弟一键直达。\n"
     ]
    }
   ],
   "source": [
    "print entertainment[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　报道称，美国一些防务专家批评白宫错失了一些适当采取军事手段但不至于引发战争的机会。\n"
     ]
    }
   ],
   "source": [
    "print military[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　,中新网,清远1月20日电 (记者 唐贵江)2017/18“叮咚出行”广东超级杯七人制足球联赛(清远杯)暨2017青联杯足球联赛启动仪式，20日在清远清新体育馆足球场举行，中国足球协会副主席、广东省足球协会名誉主席、广东省民间足球促进会会长容志行为赛事授牌，并勉励活跃群众足球运动。\n"
     ]
    }
   ],
   "source": [
    "print sports[82]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stopwords_read  = pd.read_csv(\"./data/stopwords_NLP.txt\", index_col = False, quoting = 3, sep = \"\\t\", names = ['stopword'], encoding = 'utf-8')\n",
    "stopwords  = stopwords_read['stopword'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+' ',' '-' '--' '.' '..' '...' '......' '...................' './' '.一'\n",
      " '记者' '数' '年' '月' '日' '时' '分' '秒' '/' '//' '0' '1' '2' '3' '4' '5' '6' '7'\n",
      " '8' '9' ':' '://' '::' ';' '<' '=' '>' '>>' '?' '@' 'A' 'Lex' '[' '\\\\' ']'\n",
      " '【' '】' '^' '_' '`' 'exp' 'sub' 'sup' '|' '}' '~' '~~~~' '·' '×' '×××' 'Δ'\n",
      " 'Ψ' 'γ' 'μ' 'φ' 'φ．' 'В' '—' '——' '———' '‘' '’' '’‘' '“' '”' '”，' '…' '……'\n",
      " '…………………………………………………③' '′∈' '′｜' '℃' 'Ⅲ' '↑' '→' '∈［' '∪φ∈' '≈' '①' '②'\n",
      " '②ｃ' '③' '③］' '④' '⑤' '⑥' '⑦' '⑧' '⑨' '⑩' '──']\n"
     ]
    }
   ],
   "source": [
    "print (stopwords[10:112])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Text Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'technology' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-770534760052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# preprocess the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtechnology\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'technology'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'car'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentertainment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entertainment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'technology' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category,stopwords):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwords, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\"__label__\"+str(category)+\", \"+\" \" .join(segs))\n",
    "        except Exception as e:\n",
    "            print (\"Exception infor\",e)\n",
    "            #print line\n",
    "            continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences,cate_dic['technology'],stopwords)\n",
    "preprocess_text(car,sentences,cate_dic['car'],stopwords)\n",
    "preprocess_text(entertainment,sentences,cate_dic['entertainment'],stopwords)\n",
    "preprocess_text(military,sentences,cate_dic['military'],stopwords)\n",
    "preprocess_text(sports,sentences,cate_dic['sports'],stopwords)\n",
    "\n",
    "# 乱序处理 - 使得同一类别的样本不扎堆出现在一起\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.写入文本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format\n",
      "Done....\n"
     ]
    }
   ],
   "source": [
    "print (\"writing data to fasttext format\")\n",
    "openout  = open('training_datasets.txt','w')  # 写入的方式打开\n",
    "\n",
    "for sentence in sentences:\n",
    "    openout.write(sentence.encode('utf-8') + \"\\n\")  # 中文形式  encode('utf-8') + \"\\n\" 换行符\n",
    "print (\"Done....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.调用fasttext 训练生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named fasttext",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-16b89d498501>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_datasets.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'classifier.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_prefix\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'__label__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named fasttext"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "classifier  = fasttext.supervised('training_datasets.txt','classifier.model', label_prefix  = '__label__') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test and evaluted the data set \n",
    "testresult = classifier.test('training_datasets.txt')\n",
    "\n",
    "# printing 准确值和召回率\n",
    "print 'testresult precision', testresult.precision \n",
    "print 'testresult recall', testresult.recall \n",
    "\n",
    "print 'Number of examples: ', testresult.nexamples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.实际预测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_cata_test = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# 待测试样本\n",
    "texts  = ['马来西亚 反贪 委员会 称 马来西亚 前总理 纳吉布 被逮捕 因其牵涉']\n",
    "labels = classifier.predict(texts)\n",
    "\n",
    "#printing label and category \n",
    "print('labels is :', labels)\n",
    "print label_to_cata_test[int(labels[0][0])]\n",
    "\n",
    "\n",
    "# 同时输出有多少的概率来肯定种类是这个\n",
    "labels = classifier.predict_proba(texts)\n",
    "print labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TopK 预测结果分析\n",
    "# K = 5\n",
    "category = classifier.predict(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输出category 及其对应的概率\n",
    "category = classifier.predict_proba(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (二). 通过fasttext做无监督文本学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences,cate_dic['technology'])\n",
    "preprocess_text(car,sentences,cate_dic['car'])\n",
    "preprocess_text(entertainment,sentences,cate_dic['entertainment'])\n",
    "preprocess_text(military,sentences,cate_dic['military'])\n",
    "preprocess_text(sports,sentences,cate_dic['sports'])\n",
    "\n",
    "\n",
    "# print out \n",
    "print \"writing data to  fasttext unsupervised learning format ...\"\n",
    "writeout = open('unsupervised_trainingdatasets.txt','w')\n",
    "\n",
    "for sentence in sentences: \n",
    "    writeout.write(sentence.encode('utf-8')+ \"\\n\")\n",
    "    \n",
    "print\"write Done ...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using fasttext to training the data sets\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('unsupervised_trainingdatasets.txt','model')\n",
    "# print the list \n",
    "print model.words\n",
    "\n",
    "# CBOW model - continue bags of words \n",
    "model  = fasttext.cbow('unsupervised_trainingdatasets.txt','model')\n",
    "print model.words # list of words from dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (三). Gensim vs Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成无监督训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data - 无监督不需要标签\n",
    "preprocess_text(technology,sentences)\n",
    "preprocess_text(car,sentences)\n",
    "preprocess_text(entertainment,sentences)\n",
    "preprocess_text(military,sentences)\n",
    "preprocess_text(sports,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##model fitting \n",
    "model = Word2Vec(sentences, size = 100, window = 5, min_count = 5, workers = 4)\n",
    "model.save(gensim_word2vec.model)\n",
    "model.wv['信息']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (四).文本分类by Deep Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: \n",
    "#并不是将全部数据全部加在到内容，而是将一个batch 一个batch 学习及权重更新去学习新的模型\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.CNN 做文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used on short text processing \n",
    "# LSTM can be used to long text processing\n",
    "\n",
    "# CNN 中的filter 窗口大小跟词向量的文本大小有关，每个词的窗口可能为 词向量的个数* 每个词向量的维数\n",
    "# 窗口filter + pooling 池化 + fullconnection 全链接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing and remove stopwards \n",
    "import pandas as pd \n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sports\n",
    "df_sports   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sports   = df_sports.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"./data/military_news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n",
    "\n",
    "\n",
    "### remove stopwards\n",
    "stopwords = pd.read_csv(\"data/stopwords_NLP.txt\",index_col = False, quoting =3, sep= \"\\t\", names =['stopwords'],encoding = 'utf-8' )\n",
    "stopwords = stopwords['stopwords'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Construct Data \n",
    "# Data Preparation \n",
    "import  jieba \n",
    "def preprocess_text_cnn(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwords, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append((\" \".join(segs),category))  # 添加数据及其label\n",
    "        except Exception as e:\n",
    "            print (\">>>\",line)\n",
    "            print (\"Exception infor>>>\",e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Yazhou\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.839 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 6.000000(min)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "sentences = []\n",
    "start = time.time()\n",
    "preprocess_text_cnn(technology,sentences,'technology')\n",
    "preprocess_text_cnn(car,sentences,'car')\n",
    "preprocess_text_cnn(entertainment,sentences,'entertainment')\n",
    "preprocess_text_cnn(military,sentences,'military')\n",
    "preprocess_text_cnn(sports,sentences,'sports')\n",
    "end  = time.time()\n",
    "print (\"运行时间: %f(min)\" %(int((end - start)/60) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"分割数据集\"\n",
    "import sklearn\n",
    "\"python3\"\n",
    "from  sklearn.cross_validation import train_test_split\n",
    "\n",
    "\"python2\"\n",
    "#from sklearn.model_selection import train_test_split \n",
    "\n",
    "# 拉链 将词语和label 分别分给x and y \n",
    "x,y = zip(*sentences)\n",
    "\n",
    "# split the data into trianing and test data sets \n",
    "X_train_,X_test_,y_train,y_test   = train_test_split(x,y,random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 构建神经网络过程 - 中文文本分类 on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "import tensorflow as tf \n",
    "learn  = tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables Initialization\n",
    "FLAGS = None\n",
    "# 文档最长的长度\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "# 最小词频数\n",
    "MIN_WORD_FREQUENCY = 2 \n",
    "\n",
    "# 词嵌入的维度\n",
    "EMBEDDING_SIZE= 20\n",
    "\n",
    "# filter 数量\n",
    "N_FILTERS = 10\n",
    "\n",
    "# Windows size \n",
    "WINDOWS_SIZE = 20\n",
    "\n",
    "#filter 的形状\n",
    "FILTER_SHAPE1 = [WINDOWS_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOWS_SIZE, N_FILTERS]\n",
    "\n",
    "# Pooling \n",
    "POOLING_WINDOW  = 4\n",
    "POOLING_STRIDE  = 2 \n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define CNN Model 卷积神经网络\n",
    "def cnn_model(features,target):\n",
    "    ###\n",
    "    ### 两层的卷积神经网络，用于短文本分类\n",
    "    # 先把词转成词嵌入\n",
    "    # 我们得到一个形状为[n_words,EMBEDDING_SIZE] 的词表映射矩阵\n",
    "    # 接着我们可以把一批文本映射成[batch_size,sequence_length,EMBEDDING_SIZE]的矩阵\n",
    "    \n",
    "    # one - hot 编码 \n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    \n",
    "    # 将feature/文本 的序列做一个映射，编成一个二维向量\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(features, vocab_size = n_words, embed_dim = EMBEDDING_SIZE,scope = 'words')\n",
    "    \n",
    "    # 将2维转成3 维\n",
    "    word_vectors = tf.expand_dims(word_vectors,3)\n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        # 添加一个二维的卷积滤波\n",
    "        conv1  = tf.contrib.layers.convolution2d(word_vectors,N_FILTERS,FILTER_SHAPE1,padding = 'VALID')\n",
    "        # 添加RELU非线性 - 激活函数\n",
    "        conv1  = tf.nn.relu(conv1)\n",
    "        \n",
    "        # maxmimum pooling \n",
    "        pool1 = tf.nn.max_pool(conv1,ksize = [1,POOLING_WINDOW,1,1], strides = [1,POOLING_STRIDE,1,1], padding = 'SAME')\n",
    "        \n",
    "        # 对矩阵转置 以满足形状\n",
    "        pool1 = tf.transpose(pool1,[0,1,3,2])\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer2'):\n",
    "            # 第2个卷积层\n",
    "            conv2 = tf.contrib.layers.convolution2d(pool1,N_FILTERS,FILTER_SHAPE2,padding = 'VALID')\n",
    "            \n",
    "            # 抽取特征\n",
    "            pool2  = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims = [1])\n",
    "            \n",
    "            \n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn = None)  # 无激活函数\n",
    "    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "    \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tensorflow.preprocessing 里的VocabularyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp  = ['I am good', 'You are  here ','I am glad', 'it is great']\n",
    "#\n",
    "# 只要出现的最小频率是1 或者比1 大，就处理\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(10, min_frequency=1)\n",
    "list(vocab_processor.fit_transform(temp))\n",
    "\n",
    "# I am good -> [1,2,0,0,......] length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 67311\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)\n",
    "X_train = np.array(list(vocab_processor.fit_transform(X_train_)))\n",
    "X_test = np.array(list(vocab_processor.transform(X_test_)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' %n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_dic = {'technology':1,'car':2,'entertainment':3, 'military':4, 'sports':5}\n",
    "cate_dic['entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "# 将类别映射成数字\n",
    "import pandas \n",
    "cate_dic = {'technology':1,'car':2,'entertainment':3, 'military':4, 'sports':5}\n",
    "y_train_map = map(lambda x:cate_dic[x],y_train)\n",
    "y_test_map = map(lambda x:cate_dic[x],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ser =  pandas.Series(y_train_map)\n",
    "y_test_ser  =  pandas.Series(y_test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train_list = list(y_train_map)\n",
    "#y_test_list  = list(y_test_map)\n",
    "#y_train_ser =  pandas.Series(y_train_list)\n",
    "#y_test_ser  =  pandas.Series(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_test_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_map>>>  <map object at 0x00000000201BA2B0>\n",
      "xy_map>>>  <map object at 0x00000000201BA2B0>\n",
      "xy_map>>>  <map object at 0x00000000201BA2B0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n('output>>> ', [4, 10, 18])\\n('output>>> ', [4, 10, 18])\\n('output>>> ', [4, 10, 18])\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_map = map(lambda x,y:x*y,[1,2,3],[4,5,6])\n",
    "for i in xy_map:\n",
    "    print (\"xy_map>>> \",xy_map)### Python3中，这个是迭代器，不会打印出结果\n",
    "\n",
    "\"下面是在python2中运行的结果：\"\n",
    "\"\"\"\n",
    "('output>>> ', [4, 10, 18])\n",
    "('output>>> ', [4, 10, 18])\n",
    "('output>>> ', [4, 10, 18])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#type(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001EC11438>, '_save_summary_steps': 100, '_tf_random_seed': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_master': '', '_task_id': 0, '_num_worker_replicas': 0, '_model_dir': None, '_environment': 'local', '_keep_checkpoint_every_n_hours': 10000}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpgvw61xsd\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpgvw61xsd\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7083066, step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.46477\n",
      "INFO:tensorflow:loss = 0.9036725, step = 101 (11.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.52684\n",
      "INFO:tensorflow:loss = 0.45848575, step = 201 (11.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.64031\n",
      "INFO:tensorflow:loss = 0.44586396, step = 301 (11.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.23134\n",
      "INFO:tensorflow:loss = 0.38648105, step = 401 (12.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.14108\n",
      "INFO:tensorflow:loss = 0.34933153, step = 501 (10.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.43075\n",
      "INFO:tensorflow:loss = 0.2468107, step = 601 (10.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.39973\n",
      "INFO:tensorflow:loss = 0.21608669, step = 701 (10.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.91215\n",
      "INFO:tensorflow:loss = 0.29764503, step = 801 (11.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.413\n",
      "INFO:tensorflow:loss = 0.2805065, step = 901 (10.623 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpgvw61xsd\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2330238.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpgvw61xsd\\model.ckpt-1000\n",
      "Accuracy:0.876761\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn = cnn_model))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(X_train,y_train_ser,steps = 1000)\n",
    "y_predicted = classifier.predict(X_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test_ser,y_predicted)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 使用LSTM -词袋模型来完成文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用RNN完成文本分类\n",
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "import tensorflow as tf \n",
    "from tensorflow.contrib.layers.python.layers import encoders \n",
    "learn  = tf.contrib.learn\n",
    "FLAGS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 67311\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001ECCBF28>, '_save_summary_steps': 100, '_tf_random_seed': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_master': '', '_task_id': 0, '_num_worker_replicas': 0, '_model_dir': None, '_environment': 'local', '_keep_checkpoint_every_n_hours': 10000}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpqnd560jp\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'features' has DataType int32 not in list of allowed values: float16, float32, float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3ec3764081ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSKCompat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[1;31m# 训练和预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_ser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, steps, max_steps, monitors)\u001b[0m\n\u001b[1;32m   1315\u001b[0m                         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m                         \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m                         monitors=all_monitors)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    283\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0mmodel_fn_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       all_hooks.extend([\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \"\"\"\n\u001b[0;32m-> 1132\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'model_dir'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_dir'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-3ec3764081ce>\u001b[0m in \u001b[0;36mbag_of_words_model\u001b[0;34m(features, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[1;31m# 预测值 : logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 无激活函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#target:真实值， logits；预测值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(onehot_labels, logits, weights, label_smoothing, scope, loss_collection)\u001b[0m\n\u001b[1;32m    527\u001b[0m     losses = nn.softmax_cross_entropy_with_logits(labels=onehot_labels,\n\u001b[1;32m    528\u001b[0m                                                   \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                                                   name=\"xentropy\")\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompute_weighted_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_collection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1644\u001b[0m   \u001b[1;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1646\u001b[0;31m       precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m   \u001b[1;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   2303\u001b[0m   \"\"\"\n\u001b[1;32m   2304\u001b[0m   result = _op_def_lib.apply_op(\"SoftmaxCrossEntropyWithLogits\",\n\u001b[0;32m-> 2305\u001b[0;31m                                 features=features, labels=labels, name=name)\n\u001b[0m\u001b[1;32m   2306\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    588\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    589\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[1;34m\"allowed values: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'features' has DataType int32 not in list of allowed values: float16, float32, float64"
     ]
    }
   ],
   "source": [
    "# 通过词袋模型来一批一批的把数据灌进去\n",
    "MAX_DOCUMENT_LENGTN = 15\n",
    "MIN_WORD_FREQUENCE  = 1\n",
    "EMBEDDING_SIZE      = 50\n",
    "\n",
    "global n_words\n",
    "# 处理词汇\n",
    "#vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_train_)))\n",
    "x_test = np.array(list(vocab_processor.transform(X_test_)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d'%n_words)\n",
    "\n",
    "def bag_of_words_model(features,target):\n",
    "    # 生成词袋模型\n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    features = encoders.bow_encoder(features,vocab_size = n_words,embed_dim = EMBEDDING_SIZE)\n",
    "\n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    logits = tf.contrib.layers.fully_connected(features, 15, activation_fn = None)  # 无激活函数\n",
    "    loss   = tf.losses.softmax_cross_entropy(logits, target)  #target:真实值， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "  \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n",
    "\n",
    "#\n",
    "model_fn = bag_of_words_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn = model_fn))\n",
    "# 训练和预测\n",
    "classifier.fit(x_train,y_train_ser,steps = 1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test_ser,y_predicted)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 67311\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001E2BBBE0>, '_save_summary_steps': 100, '_tf_random_seed': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_master': '', '_task_id': 0, '_num_worker_replicas': 0, '_model_dir': None, '_environment': 'local', '_keep_checkpoint_every_n_hours': 10000}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpy900907x\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'indices' has DataType string not in list of allowed values: uint8, int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5b9bbe04e620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;31m# Train and predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, steps, max_steps, monitors)\u001b[0m\n\u001b[1;32m   1315\u001b[0m                         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m                         \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m                         monitors=all_monitors)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    283\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0mmodel_fn_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       all_hooks.extend([\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \"\"\"\n\u001b[0;32m-> 1132\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'model_dir'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_dir'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-5b9bbe04e620>\u001b[0m in \u001b[0;36mbag_of_words_model\u001b[0;34m(features, target)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbag_of_words_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[1;34m\"\"\"先转成词袋模型\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \tfeatures = encoders.bow_encoder(\n\u001b[1;32m     30\u001b[0m \t\t\tfeatures, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m     return gen_array_ops._one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0;32m-> 2156\u001b[0;31m                                   name)\n\u001b[0m\u001b[1;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_one_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   1846\u001b[0m   result = _op_def_lib.apply_op(\"OneHot\", indices=indices, depth=depth,\n\u001b[1;32m   1847\u001b[0m                                 \u001b[0mon_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moff_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moff_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m                                 axis=axis, name=name)\n\u001b[0m\u001b[1;32m   1849\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    588\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    589\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[1;34m\"allowed values: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'indices' has DataType string not in list of allowed values: uint8, int32, int64"
     ]
    }
   ],
   "source": [
    "\"先用词袋模型来试试模型\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "learn = tf.contrib.learn\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 15\n",
    "MIN_WORD_FREQUENCE = 1\n",
    "EMBEDDING_SIZE = 50\n",
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_train_)))\n",
    "x_test = np.array(list(vocab_processor.transform(X_test_)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)\n",
    "\n",
    "def bag_of_words_model(features, target):\n",
    "\t\"\"\"先转成词袋模型\"\"\"\n",
    "\ttarget = tf.one_hot(target, 15, 1, 0)\n",
    "\tfeatures = encoders.bow_encoder(\n",
    "\t\t\tfeatures, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\tlogits = tf.contrib.layers.fully_connected(features, 15, activation_fn=None)\n",
    "\tloss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\ttrain_op = tf.contrib.layers.optimize_loss(\n",
    "\t\t\tloss,\n",
    "\t\t\ttf.contrib.framework.get_global_step(),\n",
    "\t\t\toptimizer='Adam',\n",
    "\t\t\tlearning_rate=0.01)\n",
    "\treturn ({\n",
    "\t\t\t'class': tf.argmax(logits, 1),\n",
    "\t\t\t'prob': tf.nn.softmax(logits)\n",
    "\t}, loss, train_op)\n",
    "\n",
    "model_fn = bag_of_words_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train_ser, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GRU分类器的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(features, target):\n",
    "\t\"\"\"用RNN模型(这里用的是GRU)完成文本分类\"\"\"\n",
    "\t# Convert indexes of words into embeddings.\n",
    "\t# This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "\t# maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "\t# EMBEDDING_SIZE].\n",
    "\tword_vectors = tf.contrib.layers.embed_sequence(\n",
    "\t\t\tfeatures, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\n",
    "\t# Split into list of embedding per word, while removing doc length dim.\n",
    "\t# word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "\tword_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "\t# Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "\tcell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "\t# Create an unrolled Recurrent Neural Networks to length of\n",
    "\t# MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "\t_, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "\t# Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "\t# neural network of last step) and pass it as features for logistic\n",
    "\t# regression over output classes.\n",
    "\ttarget = tf.one_hot(target, 15, 1, 0)\n",
    "\tlogits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "\tloss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "\t# Create a training op.\n",
    "\ttrain_op = tf.contrib.layers.optimize_loss(\n",
    "\t\t\tloss,\n",
    "\t\t\ttf.contrib.framework.get_global_step(),\n",
    "\t\t\toptimizer='Adam',\n",
    "\t\t\tlearning_rate=0.01)\n",
    "\n",
    "\treturn ({\n",
    "\t\t\t'class': tf.argmax(logits, 1),\n",
    "\t\t\t'prob': tf.nn.softmax(logits)\n",
    "\t}, loss, train_op)\n",
    "\n",
    "model_fn = rnn_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### 使用RNN（这里使用的是GRU）来完成文本的分类工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
