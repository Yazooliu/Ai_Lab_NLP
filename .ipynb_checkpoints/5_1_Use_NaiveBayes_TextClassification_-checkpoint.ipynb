{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "#  Purpose:\n",
    "#        Using NaiveBayes to do text classification - MultinomialNB \n",
    "#  Modify History:\n",
    "#        2018 - 12 - 20 \n",
    "#  Proposer :\n",
    "#        \n",
    "#  python version: 3\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Modeling Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#python 3.x\n",
    "import os \n",
    "import time \n",
    "import random\n",
    "import jieba\n",
    "import sklearn \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np \n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.出掉stopwords中的重复性的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_deplicated_words(words):\n",
    "    words_set = set()  # 利用set 的属性仅存储单一的元素\n",
    "    with open(words,'r',encoding = 'utf-8') as readwords:\n",
    "        for line in readwords.readlines():\n",
    "            word  = line.strip()  # word is string \n",
    "            if len(word)>0 and word not in words_set: # 没有出现再现有的words 集合set 中则，增加进来\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 读取变量的大小及类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取size 大小\n",
    "import numpy as np\n",
    "def GetInputSize(input):\n",
    "    input   = list(input)\n",
    "    inputs  = np.array(input)\n",
    "    input_size = inputs.shape\n",
    "    print(\"该变量的大小是 :\",input_size)\n",
    "    \n",
    "def GetInputType(input):\n",
    "    print(\"该变量类型是:\",type(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.生成数据样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Examples_Generated(folder_path,test_dataset_percentage = 0.30):\n",
    "    folder_list = os.listdir(folder_path)              # listdir - 用于列出该路劲下所有的文件及文件夹\n",
    "    data_list   = []\n",
    "    label_list  = []\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path,folder)\n",
    "        files           = os.listdir(new_folder_path)  # 读取每个文件夹下面的全部文件\n",
    "        \n",
    "        # Reading Files\n",
    "        jindex =  1 # index initialization \n",
    "        for file in files:\n",
    "            # worry about memory if >100:\n",
    "            if jindex >= 100:\n",
    "                print(\"文件数目大于100,读取停止!\")\n",
    "            \n",
    "            # reading file from detail file path \n",
    "            with open(os.path.join(new_folder_path,file),'r') as open_handel:\n",
    "                read_rawdata = open_handel.read()\n",
    "            \n",
    "            # jieba cut - 分词\n",
    "            rawdata_cut      = jieba.cut(read_rawdata,cut_all = False)  # cut 精简model \n",
    "            rawdata_cut_list = list(rawdata_cut)\n",
    "            \n",
    "            data_list.append(rawdata_cut_list) # data \n",
    "            label_list.append(folder) # folder name is the label of the data in this folder\n",
    "            # label_list.append(folder.decode('utf-8'))\n",
    "            jindex += 1 \n",
    "    \n",
    "    # divide the dataset into train and test data manual \n",
    "    data_label_list = list(zip(data_list,label_list))# 将两个list以元祖的形式组合成list = [(data_list, label_list)]\n",
    "    random.shuffle(data_label_list)  # 乱序\n",
    "    \n",
    "    # 手动区分 - Index_boundary \n",
    "    index_boundary = int(len(data_label_list)* test_dataset_percentage) \n",
    "    test_dataset   = data_label_list[0:index_boundary]\n",
    "    train_dataset  = data_label_list[index_boundary: ]\n",
    "    \n",
    "    # 通过zip(*) 将原来组合的list 再重写解开成两个单独的list, one is data, other one is list \n",
    "    test_data,test_data_lable   = list(zip(*test_dataset))\n",
    "    train_data,train_data_lable = list(zip(*train_dataset))\n",
    "    \n",
    "    # transform to list type\n",
    "    train_data_lable = list(train_data_lable)\n",
    "    test_data_lable  = list(test_data_lable)\n",
    "    test_data  = list(test_data)\n",
    "    train_data = list(train_data)\n",
    "   \n",
    "    # 统计词频方法all_words_dict 中\n",
    "    statics_wordsfrequency_dict  = {}\n",
    "    for word in train_data:     # every single 'word' in train_data \n",
    "        for character in word:  # every single 'character' in word\n",
    "            if character in statics_wordsfrequency_dict:  # python3.x deleted has_key, has_key only used in python 2.x\n",
    "                statics_wordsfrequency_dict[character] += 1   # dict[key] = value 赋值方法！\n",
    "            else:\n",
    "                statics_wordsfrequency_dict[character]  = 1\n",
    "\n",
    "    # key = lambda x:x[1] - 按照每个元素的第二值即value 排序，reverse =True 逆序-降序排列\n",
    "    order_statics_list = sorted(statics_wordsfrequency_dict.items(),key = lambda x:x[1], reverse =True) \n",
    "    all_words_list     = list(list(zip(*order_statics_list))[0]) # zip(*) 要转成list in python 3.x \n",
    "    \n",
    "    # return 想要的结果\n",
    "    return all_words_list, train_data, test_data, train_data_lable, test_data_lable  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.提取有价值的特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction (all_words_list,deletN, stopwords_set = set() ):\n",
    "    # feature extraction \n",
    "    feature_words =  []\n",
    "    n             =   1  \n",
    "    for t in range(deletN,len(all_words_list),1):\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1< len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features(train_data,test_data,feature_words,flag = 'nltk'):\n",
    "    def text_features(text,feature_words):\n",
    "        text_words = set(text)\n",
    "        ##---------\n",
    "        if flag   == 'nltk':\n",
    "            features = {word: 1 if word in text_words else 0 for word in feature_words }\n",
    "            \n",
    "        elif flag == 'sklearn':\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ##---------\n",
    "        return features\n",
    "    \n",
    "    train_feature_list  =  [text_features(text,feature_words) for text in train_data]\n",
    "    test_feature_list   =  [text_features(text,feature_words) for text in test_data]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model_classificaton(train_feature_list,test_feature_list,train_data_lable,test_data_lable,flag = 'nltk'):\n",
    "    ## -----\n",
    "    if flag == 'nltk':\n",
    "        ## 使用nltk分类器\n",
    "        train_list = zip(train_feature_list,train_data_lable)\n",
    "        test_list  = zip(test_feature_list,test_data_lable)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_list) # NaiveBayesClassifier  \n",
    "        accuracy_test  = nltk.classify.accuracy(classifier,test_list)\n",
    "    # -----\n",
    "    elif flag == 'sklearn':\n",
    "        classifier = MultinomialNB().fit(train_feature_list,train_data_lable)\n",
    "        accuracy_test  = classifier.score(test_feature_list,test_data_lable)\n",
    "    else:\n",
    "        accuracy_test  = []\n",
    "    return accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ... \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-65acce1b0eb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Text pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./data/Database/SogouC/Sample\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mall_words_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_lable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_lable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExamples_Generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataset_percentage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Using sklearn to split dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-bead5fa2e8d8>\u001b[0m in \u001b[0;36mExamples_Generated\u001b[1;34m(folder_path, test_dataset_percentage)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# reading file from detail file path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_folder_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopen_handel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mread_rawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_handel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "print(\"Starting ... \")\n",
    "\n",
    "# Text pre-processing \n",
    "folder_path = \"./data/Database/SogouC/Sample\"\n",
    "all_words_list,train_data,test_data,train_data_lable,test_data_lable = Examples_Generated(folder_path,test_dataset_percentage = 0.20)\n",
    "\n",
    "# Using sklearn to split dataset  \n",
    "#rom sklearn.model_selection import train_test_split\n",
    "#x_trian,x_test,y_train,y_test = train_test_split(train_data,test_data,random_state = 200)\n",
    "\n",
    "# Generated stop words\n",
    "stopwords_txtfile  = './data/stopwords_cn_in5NavBayesTextClassifier.txt'\n",
    "stopwords_set      = remove_deplicated_words(stopwords_txtfile)\n",
    "\n",
    "#all_words_list     = np.array(all_words_list) # 转成array - \n",
    "#print(type(all_words_list))\n",
    "#all_words_reshaped = all_words_list.reshape(1,len(all_words_list))  # 转成1行，X列\n",
    "\n",
    "# Get Variable Size \n",
    "#GetInputSize(all_words_list)\n",
    "#GetInputSize(train_data)\n",
    "#GetInputSize(train_data_lable)\n",
    "#GetInputSize(test_data)\n",
    "#GetInputSize(test_data_lable)\n",
    "\n",
    "# Text Extraction and Classification \n",
    "flag      = 'sklearn'\n",
    "deleteNs  = range(0,2300,20)\n",
    "accuracy_testscore = []\n",
    "for deleteN in deleteNs:\n",
    "    feature_words      =   feature_extraction(all_words_list,deleteN,stopwords_set)\n",
    "    train_feature_list, test_feature_list = text_features(train_data,test_data,feature_words,flag)\n",
    "    accuracy_test      =  text_model_classificaton(train_feature_list,test_feature_list,train_data_lable,test_data_lable,flag)\n",
    "    accuracy_testscore.append(accuracy_test)\n",
    "\n",
    "print(\"测试准确率是:\" , accuracy_testscore)\n",
    "\n",
    "# Result \n",
    "plt.figure()\n",
    "plt.plot(deleteNs,accuracy_testscore)\n",
    "plt.title(\"relationship between deleteNs and accuracy_testscore\")\n",
    "plt.xlabel(\"deleteNs\")\n",
    "plt.ylabel(\"accuracy_testscore\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
