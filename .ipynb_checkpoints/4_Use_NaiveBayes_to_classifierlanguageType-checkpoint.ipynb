{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------------\n",
    "#  Modify Date:\n",
    "#         2018 - 12 -2 \n",
    "#  利用朴素贝叶斯分类器实现简单语言种类分类器\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 数据预处理 - 获取训练和测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"index = 0\\nfor line in lines:\\n    if line.strip()[-2:] == 'es':\\n        index += 1\\n        if index == 1:\\n            print(line.strip()[:-3])\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入数据并做简单处理\n",
    "in_f  = open('./data/data.csv')\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "\n",
    "# 数据和标签单独拿出来  data：[:-3], label: [-2:]\n",
    "dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]  # 先取出空格strip 然后再取索引\n",
    "\n",
    "# split dataset into test and training dataser\n",
    "import sklearn\n",
    "x,y    = zip(*dataset)  # dataset type is the list \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 100)\n",
    "# len(x_train) #  => 6799 same with len(y_train)\n",
    "\n",
    "'''index = 0\n",
    "for line in lines:\n",
    "    if line.strip()[-2:] == 'es':\n",
    "        index += 1\n",
    "        if index == 1:\n",
    "            print(line.strip()[:-3])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday pour les fans du grand d茅tournement vous pouvez suivre laclasse',\n",
       " 'wo bebt die erde twitter weiss']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 剔除噪音数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过正则表达式来剔除噪声\n",
    "import re \n",
    "def noise_remove(inputs):\n",
    "    pattern     = re.compile(\" \".join([\"@\\w+\",'#\\w+','http\\S+']) )\n",
    "    cleand_text = re.sub(pattern, \" \",inputs)\n",
    "    return cleand_text.strip()\n",
    "\n",
    "#noise_remove(\"Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html\")\n",
    "# >>> 'Trump images are now more popular than cat gifs.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 下一步要做的是抽取出有用的特征，抽取1-gram and 2-gram 的统计特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countervectorizer = CountVectorizer(\n",
    "    lowercase    = True,        # lowercase the text \n",
    "    analyzer     = 'char_wb',   # tokenise by character ngrams \n",
    "    ngram_range  = (1,2),       # ngrams : 1-gram and 2-gram 统计特征\n",
    "    max_features = 1000,        # only consider the top max_features ordered \n",
    "    preprocessor = noise_remove # preprocess - 预处理\n",
    ")\n",
    "# Instraction for CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 建立分类器 - 并计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6799,)\n",
      "(6799,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6560d3eb4a39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 6799*1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 6799*1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "# import the clasifier\n",
    "from sklearn.naive_bayes import MultinomialNB # 多项式朴素贝叶斯\n",
    "classifier = MultinomialNB()\n",
    "'''import numpy as np \n",
    "print(np.array(x_train).shape)  # 6799*1 \n",
    "print(np.array(y_train).shape)  # 6799*1 '''\n",
    "\n",
    "classifier.fit(countervectorizer.fit_transform(x_train),y_train) # fit_transform 对训练集先拟合然后再归一化\n",
    "\n",
    "# test score on test dataset - 准确率\n",
    "print('score is: %s' %(classifier.score(countervectorizer.transform(x_test), y_test))) # transform 对测试集归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  规范化：写成一个类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class LanguageClassifierByNaiveBayes():\n",
    "    \n",
    "    def __init__(self,classifier = MultinomialNB() ):  # __init__ 函数默认被调用\n",
    "        self.classifier = classifier\n",
    "        self.countervectorizer  = CountVectorizer(\n",
    "                   lowercase    = True,      # lowercase the text \n",
    "                   analyzer     = 'char_wb', # tokenise by character ngrams \n",
    "                   ngram_range  = (1,4),     # ngrams : 1-gram and 2-gram 统计特征\n",
    "                   max_features = 1000,      # only consider the top max_features ordered \n",
    "                   preprocessor = self.remove_noise # preprocess - 预处理\n",
    "                   )\n",
    "    # remove noise \n",
    "    def remove_noise(self,inputs):\n",
    "        pattern      = re.compile(\"\".join([\"http\\S+\",\"\\@\\w+\",\"\\#\\w+\"]))\n",
    "        cleaned_text = re.sub(pattern,\" \",inputs)\n",
    "        return cleaned_text.strip()\n",
    "    \n",
    "    # data pre-processing:\n",
    "    def data_preprocesing(self):\n",
    "        open_data = open('./data/data.csv')\n",
    "        readlines = open_data.readlines()\n",
    "        dataset_withlabel = [(line.strip()[:-3], line.strip()[-2:]) for line in readlines] \n",
    "        x,y       =  zip(*dataset_withlabel)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 100)\n",
    "        return x_train,x_test,y_train,y_test\n",
    "    \n",
    "    # 数据归一化处理\n",
    "    def data_transform(self,data):     # 针对训练数据先归一化\n",
    "        return self.countervectorizer.transform(data)\n",
    "    \n",
    "    # 归一化处理后再拟合\n",
    "    def data_transform_fit(self,data): # 后拟合，针对测试数据直接拟合无需归一化处理\n",
    "        self.countervectorizer.fit(data)\n",
    "    \n",
    "    # 用分类器拟合数据\n",
    "    def model_fitting(self,data_x,data_y):\n",
    "        self.data_transform_fit(data_x)\n",
    "        self.classifier.fit(self.data_transform(data_x),data_y)\n",
    "    \n",
    "    def test_score(self,data_x,data_y):\n",
    "        return self.classifier.score(self.data_transform(data_x),data_y)\n",
    "    \n",
    "    def prediction(self,data):\n",
    "        return self.classifier.predict(self.data_transform([data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socre is 0.9907366563740626\n",
      "['en']\n"
     ]
    }
   ],
   "source": [
    "language_detector = LanguageClassifierByNaiveBayes()\n",
    "x_train,x_test,y_train,y_test = language_detector.data_preprocesing()\n",
    "language_detector.model_fitting(x_train,y_train)\n",
    "print(\"socre is %s\" %language_detector.test_score(x_test,y_test))\n",
    "print(language_detector.prediction('This is one football games!'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
