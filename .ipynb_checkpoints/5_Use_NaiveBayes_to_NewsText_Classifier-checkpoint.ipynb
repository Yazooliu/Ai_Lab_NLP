{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------\n",
    "# Using Naive_Bayes to do News Text Classifier \n",
    "# Modify History:\n",
    "#      2018- 12-9 \n",
    "# Author:\n",
    "#     Creator\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Modeling Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#python3 \n",
    "import os \n",
    "import time \n",
    "import random\n",
    "import jieba\n",
    "import sklearn \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np \n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.出掉stopwords中的重复性的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_deplicated_words(words):\n",
    "    words_set = set()  # 利用set 的属性仅存储单一的元素\n",
    "    with open(words,'r',encoding = 'utf-8') as readwords:\n",
    "        for line in readwords.readlines():\n",
    "            word  = line.strip()  # word is string \n",
    "            if len(word)>0 and word not in words_set: # 没有出现再现有的words 集合set 中则，增加进来\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e89a3bac1240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#folder_path = r'C:\\AI\\github\\Ai_Lab_\\Ai_Lab_NLP\\data\\Database\\SogouC\\Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/Database/SogouC/Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfolder_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;31m# listdir - 用于列出该路劲下所有的文件及文件夹\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_list\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_list\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "    #folder_path = r'C:\\AI\\github\\Ai_Lab_\\Ai_Lab_NLP\\data\\Database\\SogouC\\Sample'\n",
    "    folder_path = './data/Database/SogouC/Sample'\n",
    "    folder_list = os.listdir(folder_path)              # listdir - 用于列出该路劲下所有的文件及文件夹\n",
    "    data_list   = []\n",
    "    label_list  = []\n",
    "    #print(folder_list)\n",
    "    index = 0\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path,folder)\n",
    "        files           = os.listdir(new_folder_path)  # 读取每个文件夹下面的全部文件\n",
    "        index += 1\n",
    "        label_list.append(folder)\n",
    "        #print(\"index = %s: files= %s: it's label = %s \" %(index,files,label_list) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"F:/gts/gtsdate/\"\n",
    "b = os.path.join(path,\"abc\")\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.文本处理-用于训练及测试样本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Examples_Generated(folder_path,test_dataset_percentage = 0.20):\n",
    "    folder_list = os.listdir(folder_path)              # listdir - 用于列出该路劲下所有的文件及文件夹\n",
    "    data_list   = []\n",
    "    label_list  = []\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path,folder)\n",
    "        files           = os.listdir(new_folder_path)  # 读取每个文件夹下面的全部文件\n",
    "        \n",
    "        # Reading Files\n",
    "        jindex =  1 # index initialization \n",
    "        for file in files:\n",
    "            # worry about memory if >100:\n",
    "            if jindex >= 100:\n",
    "                print(\"jindex >= 100,Break Now!\")\n",
    "            \n",
    "            # reading file from detail file path \n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding = 'utf-8') as open_handel:\n",
    "                read_rawdata = open_handel.read()\n",
    "            \n",
    "            # jieba cut - 分词\n",
    "            rawdata_cut      = jieba.cut(read_rawdata,cut_all = False)  # cut 精简model \n",
    "            rawdata_cut_list = list(rawdata_cut)\n",
    "            \n",
    "            data_list.append(rawdata_cut_list) # data \n",
    "            label_list.append(folder) # folder name is the label of the data in this folder\n",
    "            # label_list.append(folder.decode('utf-8'))\n",
    "    \n",
    "    # divide the dataset into train and test data manual \n",
    "    data_label_list = list(zip(data_list,label_list))# 将两个list以元祖的形式组合成list = [(data_list, label_list)]\n",
    "    random.shuffle(data_label_list)  # 乱序\n",
    "    \n",
    "    # 手动区分 - Index_boundary \n",
    "    index_boundary = int(len(data_label_list)* test_dataset_percentage) \n",
    "    test_dataset   = data_label_list[0:index_boundary]\n",
    "    train_dataset  = data_label_list[index_boundary: ]\n",
    "    \n",
    "    # 通过zip(*) 将原来组合的list 再重写解开成两个单独的list, one is data, other one is list \n",
    "    test_data,test_data_lable   = list(zip(*test_dataset))\n",
    "    train_data,train_data_lable = list(zip(*train_dataset))\n",
    "    \n",
    "    # transform to list type\n",
    "    train_data_lable = list(train_data_lable)\n",
    "    test_data_lable  = list(test_data_lable)\n",
    "    test_data  = list(test_data)\n",
    "    train_data = list(train_data)\n",
    "    \n",
    "    # using sklearn train_test_split to split train and test data\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #x_trian,x_test,y_train,y_test = train_test_split(train_dataset,test_dataset,random_state = 200)\n",
    "    \n",
    "    # 统计词频方法all_words_dict 中\n",
    "    statics_wordsfrequency_dict  = {}\n",
    "    for word in train_data:     # every single 'word' in train_data \n",
    "        #print(\"word = \", word)\n",
    "        for character in word:  # every single 'character' in word\n",
    "            #print(\"character  = \", character)\n",
    "            if character in statics_wordsfrequency_dict:  # python3.x deleted has_key, has_key only used in python 2.x\n",
    "                statics_wordsfrequency_dict[character] += 1   # dict[key] = value 赋值方法！\n",
    "            else:\n",
    "                statics_wordsfrequency_dict[character]  = 1\n",
    "\n",
    "    # key = lambda x:x[1] - 按照每个元素的第二值即value 排序，reverse =True 逆序-降序排列\n",
    "    order_statics_list = sorted(statics_wordsfrequency_dict.items(),key = lambda x:x[1], reverse =True) \n",
    "    all_words_list     = list(list(zip(*order_statics_list))[0]) # zip(*) 要转成list in python 3.x \n",
    "    \n",
    "    # return 想要的结果\n",
    "    return all_words_list, train_data, test_data, train_data_lable, test_data_lable  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 提取特征词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction (all_words_list,deletN, stopwords_set = set() ):\n",
    "    # feature extraction \n",
    "    feature_words =  []\n",
    "    n             =   1  # initialization n index \n",
    "    for t in range(deletN,len(all_words_list),1):\n",
    "        if n>10000:      # feature _ words 的维数\n",
    "            print(\"n is bigger than 1000,Break !\")\n",
    "            break\n",
    "        # if all is digit, isdigit() return True \n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1< len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    \n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_features(train_data,test_data,feature_words,flag = 'nltk'):\n",
    "    def text_features(text,feature_words):\n",
    "        text_words = set(text)\n",
    "        ##---------\n",
    "        if flag   == 'nltk':\n",
    "            features = {word:  1 if word in text_words else 0 for word in feature_words }\n",
    "            \n",
    "        elif flag == 'sklearn':\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ##---------\n",
    "        return features\n",
    "    \n",
    "    train_feature_list  =  [text_features(text,feature_words) for text in train_data]\n",
    "    test_feature_list   =  [text_features(text,feature_words) for text in test_data]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 训练模型并且输出准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_model_classificaton(train_feature_list,test_feature_list,train_data_lable,test_data_lable,flag = 'nltk'):\n",
    "    ## -----\n",
    "    if flag == 'nltk':\n",
    "        ## 使用nltk分类器\n",
    "        train_list = zip(train_feature_list,train_data_lable)\n",
    "        test_list  = zip(test_feature_list,test_data_lable)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_list) # NaiveBayesClassifier  \n",
    "        accuracy_test  = nltk.classify.accuracy(classifier,test_list)\n",
    "    # -----\n",
    "    elif flag == 'sklearn':\n",
    "        #classifier     = MultinomialNB()\n",
    "        print(\"train_feature_list is \",type(train_feature_list))\n",
    "        print(\"train_data_lable is \",type(train_data_lable))\n",
    "        classifier = MultinomialNB().fit(train_feature_list,train_data_lable)\n",
    "        accuracy_test  = classifier.score(test_feature_list,test_data_lable)\n",
    "    else:\n",
    "        accuracy_test  = []\n",
    "    return accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    train_data = ['thshesieeiiiiie ','hedd','thisedusiie']\n",
    "    statics_wordsfrequency_dict  = {}\n",
    "    for word in train_data:     # every single 'word' in train_data \n",
    "        #print(\"word = \", word)\n",
    "        for character in word:  # every single 'character' in word\n",
    "            #print(\"character  = \", character)\n",
    "            if character in statics_wordsfrequency_dict:  # python3.x deleted has_key, has_key only used in python 2.x\n",
    "                statics_wordsfrequency_dict[character] += 1   # dict[key] = value 赋值方法！\n",
    "            else:\n",
    "                statics_wordsfrequency_dict[character]  = 1\n",
    "\n",
    "    # key = lambda x:x[1] - 按照每个元素的第二值即value 排序，reverse =True 逆序-降序\n",
    "    order_statics_list = sorted(statics_wordsfrequency_dict.items(),key = lambda x:x[1], reverse =True) # \n",
    "    \n",
    "    #statics_wordsfrequency_dict\n",
    "    e = list(list(zip(*order_statics_list))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - 6 Processing the Data.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ... \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Examples_Generated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b161d31bc14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Text pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./data/Database/SogouC/Sample\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_words_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_lable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_lable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExamples_Generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataset_percentage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Examples_Generated' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"Starting ... \")\n",
    "\n",
    "# Text pre-processing \n",
    "folder_path = \"./data/Database/SogouC/Sample\"\n",
    "all_words_list,train_data,test_data,train_data_lable,test_data_lable = Examples_Generated(folder_path,test_dataset_percentage = 0.20)\n",
    "\n",
    "print(len(all_words_list))\n",
    "print(type(train_data))\n",
    "print(type(test_data))\n",
    "print(type(train_data_lable))\n",
    "print(type(test_data_lable))\n",
    "\n",
    "# Generated stop words\n",
    "stopwords_txtfile = './data/stopwords_cn_in5NavBayesTextClassifier.txt'\n",
    "stopwords_set     = remove_deplicated_words(stopwords_txtfile)\n",
    "\n",
    "# Text Extraction and Classification \n",
    "flag     = 'sklearn'\n",
    "deleteN  = 1000\n",
    "accuracy_test_list = []\n",
    "#for deleteN in deleteNs:\n",
    "feature_words =   feature_extraction(all_words_list,deleteN,stopwords_set)\n",
    "train_feature_list, test_feature_list = text_features(train_data,test_data,feature_words)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\"\"\"print(np.array(train_feature_list).reshape(-1,1))\n",
    "print(np.array(train_data_lable).shape)\n",
    "print(np.array(test_feature_list).shape)\n",
    "print(np.array(test_data_lable).shape)\n",
    "\n",
    "accuracy_test =  text_model_classificaton(train_feature_list,test_feature_list,train_data_lable,test_data_lable,flag)\n",
    "accuracy_test_list.append(accuracy_test)\n",
    "print(\"Accuracy score is :\" , accuracy_test_list)\"\"\"\n",
    "\n",
    "# Result \n",
    "'''plt.figure()\n",
    "plt.plot(deleteNs,accuracy_test_list)\n",
    "plt.title(\"relationship between deleteNs and accuracy_test_list\")\n",
    "plt.xlabel(\"deleteNs\")\n",
    "plt.ylabel(\"accuracy_test_list\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Completed!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dssec=  [[1,4,43],[12,3,4]]\n",
    "#dssec.reshape(1,-1)\n",
    "dssecreshape = np.array(dssec).reshape(1,-1)\n",
    "print(np.array(dssec).reshape(1,-1))\n",
    "print(\"dssecreshape = \", dssecreshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
