{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Topic Analysis and Classificaiton by ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #嵌入中显示图片显示\n"
     ]
    }
   ],
   "source": [
    "# #  Copyright private in 2018 \n",
    "#  Modify Date: \n",
    "#          2018 - 9 - 18 \n",
    "#  Purpose : Chinese Nature language Process\n",
    "# \n",
    "# ----------\n",
    "#coding:utf-8\n",
    "__author__ = 'XXX'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import jieba\n",
    "import codecs # codecs 提供的open 方法来指定打开的文件的语言编码，他会在读取的时候\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "%matplotlib inline  #嵌入中显示图片显示\n",
    "import matplotlib \n",
    "matplotlib.rcParams['figure.figsize']  = (10.0,5.0)  # figure size 大小\n",
    "from wordcloud import WordCloud # 词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>contenttitle</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://news.sohu.com/20120612/n345428229.shtml</td>\n",
       "      <td>公安机关销毁１０余万非法枪支　跨国武器走私渐起</td>\n",
       "      <td>中广网唐山６月１２日消息（记者汤一亮　庄胜春）据中国之声《新闻晚高峰》报道，今天（１２日）上...</td>\n",
       "      <td>社会</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://news.sohu.com/20120607/n344998325.shtml</td>\n",
       "      <td>张绍刚发道歉信网友不认可：他的问题是俯视他人（图）</td>\n",
       "      <td>天津卫视求职节目《非你莫属》“晕倒门”事件余波未了，主持人张绍刚前日通过《非你莫属》节目组发...</td>\n",
       "      <td>娱乐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://news.sohu.com/20120604/n344745879.shtml</td>\n",
       "      <td>＃（关注夏收）（３）夫妻“麦客”忙麦收</td>\n",
       "      <td>临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道...</td>\n",
       "      <td>农业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.sohu.com/20120613/n345535702.shtml</td>\n",
       "      <td>欧洲杯大战在即　荷兰葡萄牙面临淘汰将背水一战</td>\n",
       "      <td>中广网北京６月１３日消息（记者王宇）据中国之声《新闻晚高峰》报道，明天凌晨两场欧洲杯的精彩比...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://news.sohu.com/20120601/n344598651.shtml</td>\n",
       "      <td>扎克伯格携妻罗马当街吃３０元麦当劳午餐（组图）</td>\n",
       "      <td>环球网记者李亮报道，正在意大利度蜜月的“脸谱”创始人扎克伯格与他华裔妻子的一举一动都处于媒体...</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url               contenttitle  \\\n",
       "0  http://news.sohu.com/20120612/n345428229.shtml    公安机关销毁１０余万非法枪支　跨国武器走私渐起   \n",
       "1  http://news.sohu.com/20120607/n344998325.shtml  张绍刚发道歉信网友不认可：他的问题是俯视他人（图）   \n",
       "2  http://news.sohu.com/20120604/n344745879.shtml        ＃（关注夏收）（３）夫妻“麦客”忙麦收   \n",
       "3  http://news.sohu.com/20120613/n345535702.shtml     欧洲杯大战在即　荷兰葡萄牙面临淘汰将背水一战   \n",
       "4  http://news.sohu.com/20120601/n344598651.shtml    扎克伯格携妻罗马当街吃３０元麦当劳午餐（组图）   \n",
       "\n",
       "                                             content label  \n",
       "0  中广网唐山６月１２日消息（记者汤一亮　庄胜春）据中国之声《新闻晚高峰》报道，今天（１２日）上...    社会  \n",
       "1  天津卫视求职节目《非你莫属》“晕倒门”事件余波未了，主持人张绍刚前日通过《非你莫属》节目组发...    娱乐  \n",
       "2  临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道...    农业  \n",
       "3  中广网北京６月１３日消息（记者王宇）据中国之声《新闻晚高峰》报道，明天凌晨两场欧洲杯的精彩比...    体育  \n",
       "4  环球网记者李亮报道，正在意大利度蜜月的“脸谱”创始人扎克伯格与他华裔妻子的一举一动都处于媒体...    科技  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('./data/news.csv',names = ['url','contenttitle','content','label'],encoding = 'utf-8')\n",
    "#df  = pd.read_csv(r'C:\\Users\\H155809\\out.csv',encoding = 'utf-8')\n",
    "#df  = pd.read_csv('C:\\\\Users\\\\H155809\\\\out.csv',encoding = 'utf-8')\n",
    "df  = df.dropna()  # drop掉可能为空的行数\n",
    "content  = df.content.values.tolist()  #取出数据中的content列，通过values tolist 转成Python列表\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道米顶村麦田间，范加江驾驶收割机在收获小麦。　三夏时节，山东小麦主产区处处可见“麦客”驾驶联合收割机在麦田中来回穿梭。生活在郯城县郯城街道东风村的范加江、赵琴兰夫妇就是众多“麦客”中的一对。小两口于２０１１年投资１１万多元购买了一台大型小麦联合收割机，成为村里第一对夫妻“麦客”。麦收时节，天一刚亮，夫妻俩就开始为农户收割小麦，中午在田间地头凑合填饱肚子，晚上有时要干到十一、二点。夫妻俩各自分工，丈夫收割，妻子负责量地、看路、买油、替农户装粮袋等。忙的时候，一天能收割６０多亩，一个麦季下来能挣２万多元。　在郯城县，有２００多对夫妻“麦客”驾驶联合收割机忙碌在田间地头。他们辛勤忙碌在麦收一线，为小麦及时归仓挥洒着辛勤的汗水，同时通过劳动也为自己带来了稳定的收入。　新华社发（张春雷　摄）\n"
     ]
    }
   ],
   "source": [
    "# df.content\n",
    "print(content[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (一) 生成词云"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.导入新闻数据及分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中广网唐山６月１２日消息（记者汤一亮　庄胜春）据中国之声《新闻晚高峰》报道，今天（１２日）上午，公安机关２０１２年缉枪制爆专项行动“统一销毁非法枪爆物品活动”在河北唐山正式启动，１０万余只非法枪支、２５０余吨炸药在全国１５０个城市被统一销毁。黄明：现在我宣布，全国缉枪制爆统一销毁行动开始！随着公安部副部长黄明一声令下，大量仿制式枪以及猎枪、火药枪、气枪在河北唐山钢铁厂被投入炼钢炉。与此同时，在全国各省区市１５０个城市，破案追缴和群众主动上缴的１０万余支非法枪支被集中销毁，在全国各指定场所，２５０余吨炸药被分别销毁。公安部治安局局长刘绍武介绍，这次销毁的非法枪支来源于三个方面。刘绍武：打击破案包括涉黑、涉恶的团伙犯罪、毒品犯罪，还有从境外非法走私的枪支爆炸物。在销毁现场，记者看到了被追缴和上缴的各式各样的枪支。刘绍武：也包括制式枪，有的是军用枪、仿制的制式抢，还有猎枪、私制的火药枪等等。按照我国的枪支管理法，这些都是严厉禁止个人非法持有的。中国是世界上持枪犯罪的犯罪率最低的国家之一。中美联手破获特大跨国走私武器弹药案近日，中美执法部门联手成功破获特大跨国走私武器弹药案，在中国抓获犯罪嫌疑人２３名，缴获各类枪支９３支、子弹５万余发及大量枪支配件。在美国抓获犯罪嫌疑人３名，缴获各类枪支１２支。这是公安部与美国移民海关执法局通过联合调查方式侦破重大跨国案件的又一成功案例。２０１１年８月２５日，上海浦东国际机场海关在对美国纽约发往浙江台州，申报品名为扩音器（音箱）的快件进行查验时，发现货物内藏有手枪９支，枪支配件９件，长枪部件７件。经检验，这些都是具有杀伤力的制式枪支及其配件。这引起了公安部和海关总署的高度重视。公安部刑侦局局长刘安成：因为是从海关进口的货物中检查出来夹带，说明来源地是境外，或是说国外，这应该是一起特大跨国走私武器弹药的案件。上海市公安局和上海海关缉私局成立联合专案组，迅速开展案件侦查。专案组于８月２６日在浙江台州ＵＰＳ取件处将犯罪嫌疑人王挺（男，３２岁，台州市人）抓获。王挺交代，他通过一境外网站上认识了上家林志富，２００９年１１月以来，林志富长期居住美国，他通过互联网组建了一个走私、贩卖、私藏枪支弹药的群体，通过网络在国内寻找枪支弹药买家，并通过美国ＵＰＳ联邦速递公司将枪支弹药从纽约快递给多名类似王挺的中间人，再通过中间人发送给国内买家。此案中，犯罪分子依托虚拟网络进行犯罪交易，隐蔽性强，涉案人员使用的身份、地址、联系方式都是虚构的，侦查难度很大。刘安成说，此案体现了是新型犯罪，特别是现代犯罪的新特点。刘安成：他不受距离的限制、经常是跨国跨境，甚至是跨一个、数个、甚至数十个国家。这种犯罪手法的改变和新型犯罪的特点，要求我们各国警方充分合作。作者：汤一亮　庄胜春</content>\n",
      "天津卫视求职节目《非你莫属》“晕倒门”事件余波未了，主持人张绍刚前日通过《非你莫属》节目组发出道歉信，称自己错在对留学生缺乏了解。但他的道歉，没有得到网友的接受和原谅，有网友尖锐指出，张绍刚的问题就在俯视他人，连道歉都不会。张绍刚：我是一番好意之前哪怕被网友骂得再凶，张绍刚也表现彪悍，声称自己没错，绝不道歉。但这几天李开复领衔讨伐节目组，网上民意汹汹要他“下课”，张绍刚显然有点撑不住。前日他通过《非你莫属》节目组，发出一则语焉不详的道歉信。信中他将自己在节目中的表现，解释为一番好意，就像几年前程鹤麟先生说他的，“追在别人屁股后面，碎嘴叨叨地说＂你得这样，这是为了你好＂，而自己的错误，在于对求职者不够了解，没有站在别人的角度考虑问题，“当不了解一个群体的时候，就无法给出准确的判断和建议，今年以来的各种沸沸扬扬，大多源自于此。”他最后表示：“留学生的批评我很感谢，我会努力去了解这个群体的所思所想。有问题的，认识、纠正，这是咱们经常跟同学们说的，今天我对自己说！”网友：别再硬挺下去张绍刚的这番道歉，却没有得到网友的认可。有网友尖锐指出，张绍刚的问题在于俯视他人，连道歉都不会：“张绍刚要么在讲台，要么在舞台，对学生、对选手都掌握生杀大权，高高在上惯了，很难做到平视。他的问题不在于对哪个群体是否了解，而是他的态度。”更有网友认为，张绍刚存在自卑感，“在所有的海归学生访谈的时候，他对于相关外文的资料，先以＂我看不懂＂推脱；当学生提及在当地生活经验的时候，他就在对方的用字里面挑刺；然后吆喝批判学生数典忘祖，不爱祖国”。有网友劝张绍刚：“张先生，请先把自卑处理，再当主持人。”“别再硬挺了，再挺下去就真成棒槌了。”（余乐）作者：余乐　（来源：羊城晚报）\n",
      "临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道米顶村麦田间，范加江驾驶收割机在收获小麦。　三夏时节，山东小麦主产区处处可见“麦客”驾驶联合收割机在麦田中来回穿梭。生活在郯城县郯城街道东风村的范加江、赵琴兰夫妇就是众多“麦客”中的一对。小两口于２０１１年投资１１万多元购买了一台大型小麦联合收割机，成为村里第一对夫妻“麦客”。麦收时节，天一刚亮，夫妻俩就开始为农户收割小麦，中午在田间地头凑合填饱肚子，晚上有时要干到十一、二点。夫妻俩各自分工，丈夫收割，妻子负责量地、看路、买油、替农户装粮袋等。忙的时候，一天能收割６０多亩，一个麦季下来能挣２万多元。　在郯城县，有２００多对夫妻“麦客”驾驶联合收割机忙碌在田间地头。他们辛勤忙碌在麦收一线，为小麦及时归仓挥洒着辛勤的汗水，同时通过劳动也为自己带来了稳定的收入。　新华社发（张春雷　摄）\n",
      "中广网北京６月１３日消息（记者王宇）据中国之声《新闻晚高峰》报道，明天凌晨两场欧洲杯的精彩比赛上演，死亡之组Ｂ组当中两支传统的强队荷兰队和葡萄牙队正面临着提前淘汰出局的危险。第一轮之后Ｂ组的积分的形势是３、３、０、０，德国和丹麦３分，葡萄牙和荷兰０分。今天又是３分的两支球队分别对阵０分的两支球队，如果０分的再输就会被淘汰，而３分的如果再赢就可以提前出线。换句话说，德国和丹麦今天晚上赢球就能出线，葡萄牙和荷兰如果输球就要回家。葡萄牙：Ｃ罗输球后失声痛哭北京时间今天晚上１２点丹麦对葡萄牙的比赛将在乌克兰的利沃夫进行，明天凌晨２点４５分德国对荷兰的比赛将在乌克兰的哈尔科夫进行。在输球就要回家的阴影笼罩之下，荷兰和葡萄牙队身上的压力可想而知。第一轮比赛中葡萄牙队负于德国。不过德国的实力强于葡萄牙强，葡萄牙队受到的质疑并不算大，但是头号球星罗纳尔多对自己的要求非常严格，现在全队的压力有七成都集中在了罗纳尔多身上，他想在国家队证明自己的欲望也非常强烈。虽然据说输球之后他甚至失声痛哭，但他后来表示在与皇马的教练穆里尼奥谈过之后，他相信葡萄牙队一定能够赢得这场球，带领球队走出困境。荷兰：媒体曝球队＂内讧＂和葡萄牙队相比，荷兰队在第一场输球以后产生的负能量更多一些。早在和丹麦队比赛之前就有媒体传出荷兰队出现了内讧，核心的人物就是范佩西和亨特拉尔这两名主力前锋的竞争者。而输给丹麦之后矛盾越来越多，涉及到的人也越来越多，有球员和球员之间的，也有球员和教练之间的。亨特拉尔对自己不能首发上场不满意，范德法特也抵触替补的角色，中场的德容质疑主帅是任人唯亲，在第一场里边把自己提前换下去没有把自己的女婿范博梅尔提前换下去。对于这样种种的矛盾，中场大将斯内德表示队中确实有人太过于自我，承认目前球队的气氛不如两年前，但是他也认为输掉第一场比赛之后出现这样的情况是正常的，但是不管怎么说如果荷兰想要战胜强大的德国队，必须先整理好内部。对于球迷来说，今天晚上的比赛会非常有吸引力，在荷兰和葡萄牙必须赢球的情况下，死亡之组的比赛将不会像首轮那样沉闷，一定会精彩纷呈。\n",
      "环球网记者李亮报道，正在意大利度蜜月的“脸谱”创始人扎克伯格与他华裔妻子的一举一动都处于媒体的追踪之下。５月３１日，在罗马这个拥有１３家米其林星级餐厅的城市，身家近千亿的扎克伯格夫妇被发现穿着普通的Ｔ恤短袖，在路边一家麦当劳花３英镑买了两人的午餐，并坐在街边的台阶上大嚼起来。英国《每日电讯报》５月３１日报道，扎克伯格夫妇的这顿午饭包括汉堡、炸鸡，仅３英镑（约为人民币３０元）。两人先是依偎着看餐牌，然后由妻子点餐，扎克伯格则调皮地拿起手机，给妻子拍照。拎着装满快餐食品的纸袋子，这对明星夫妇竟坐在街边的一个台阶上，边聊天边大吃起来。一位路人告诉记者，“没人认出他们俩。他们只是混在人群中。”延伸阅读：网友在微博上热传，ＣＣＴＶ最近播出的纪录片《中国警察》中，一位疑似脸谱网站创始人扎克伯格的群众演员成最大牌的“路人甲”。据报道，《中国警察》第四集第３１秒左右，出现貌似扎克伯格及其妻子今年３月在上海游玩时候的镜头。对着镜头，“扎克伯格”似乎笑得很开心，走路一甩一甩的让网友大呼喜感。［详细］（中国新闻网）\n",
      "本报记者　张忠德本报通讯员　张艳　苏婧城区“十分钟”从５月中旬开始，在胶南市珠海街道办事处的烟台东社区大舞台上，一台台京剧演出点燃了附近居民的热情，１００多人的演出队伍更是让几千人次的观众大呼过瘾。烟台东社区一位负责人告诉记者：“我们这个大舞台总共投资了６００多万元，可以承接很多大型演出。除了这个中心舞台外，还建设有古香古色的博物馆、村级阅览室、活动室等，每到夏天都会有很多演出。”２０１２年，胶南市继续对现有公共文化服务设施实施提升工程。其中，投资１５０万元对２０处城市社区文化中心实施功能建设完善；加大人民路、石桥大厦、珠海路三处文化广场的文化设施投资力度；在东部水城和灵山卫、北部隐珠和临港开发区、南部大学城、西部老城区等重点区域筹建具备一定演出能力的大型文化广场；加强市文化馆、博物馆、图书馆等公共文化服务设施建设和服务等，基本构建起城区“十分钟公共文化服务圈”。而在十分钟的文化圈里，一些地标性的文化工程也在逐步展开。总投资６亿元的胶南市市民文化中心目前已经通过规划，总占地达到７５亩，建筑面积１１．７万平方米，集会展中心、图书馆、博物馆、档案馆等文化馆所为一体，即将于近期开工。镇村“三公里”城区十分钟文化圈的基本建成，让胶南市将公共文化设施和服务体系建设的重心进一步延伸至基层。２０１２年，胶南市计划投资５１６万元加大基础薄弱村文化设施的建设力度：投资１８０万元加快建设经济开发区、度假区、王台镇等六个镇级特色文化广场，并根据每个特色文化广场的实际情况，配置流动舞台、灯光、音响等文化设施设备；投资５１６万元改扩建１７２处农村文化活动室，为其配置图书橱、ＤＶＤ、桌椅、音响、图书、投影仪、锣鼓等文化设施；采取以点带面、镇扶持的办法，重点培植市１４５个村（社区）特色化规范化文化活动场所，形成镇村“三公里公共文化服务圈”。胶南市在青岛市连续三年投资２９８０万元基础上，连续两年投资１１１６万元用于对镇村文化活动室的改善提升，到目前，全市所有镇街道已建成设施齐全、功能完善的综合文化站，其中一级站４处。全市１０１６个村（社区）全部建有文化活动室，其中高标准的８６０个，达到全市总村数８５％，基本能够满足镇村居民的“三公里文化需求”。传导效应显现２０１１年１０月，胶南市图书馆分馆在灵山卫街道办事处珠山文苑社区建立了第一个分馆，胶南市文广新局工作人员薛凯告诉记者：“可不要小瞧了这个分馆，虽然占地面积仅有１００多平方米，藏书也只有５０００多册，但纳入市图书馆统一目录的这个分馆实现了与胶南市图书馆的互联，村民可以实现统借统还，不必跑那么远的路去借书还书，目前这样的分馆还有一处在规划中。”下一步，为满足群众对电子文化信息的需求，胶南市将采取与党员干部现代远程教育、中小学校园网合作共建的办法，投资近１００万元推进文化信息资源共享工程建设。其中，投资７０万元在胶南市图书馆设立了文化信息资源共享工程支中心，在市级之中心示范带动下，全市建成镇村文化信息资源共享工程基层点９５５个，其中１８个达到规范化站点标准；投资２０万元建成４１个村级公共电子阅览室，２０１２年将大力实施公共电子阅览室建设工程，力争实现１０处镇街道和１９３个村公共电子资源阅览室标准化建设。为将有限的文化资源盘活起来，胶南市还坚持采用“送文化”与“种文化”相结合的方式，不断提升文化创新力。近年来，胶南持续开展了文化下乡和送欢乐下基层活动，每年财政投资３６０万元在全市所有镇（街道）和行政村（社区）开展送戏、送电影下乡活动，确保送戏下乡１５０场、放映公益电影１．２万场。除此之外，扎实推进农村益民书屋、流动文化服务进农村、进社区等文化惠民工程落到实处，保障基层特别是偏远地区群众的基本文化权益。“送文化”的同时，积极引导基层文化团体和文艺骨干“种文化”，按照鼓励发展，重点扶持的原则，着重培植１０个村级演出水平高、人员规模大、装备齐全的优秀庄户剧团，带动全市基层群众文化活动的提升，繁荣农村文化市场。　（来源：大众日报）\n",
      "中新网６月８日电　据朝中社报道，２６０多名朝鲜少年团代表７日参加了朝鲜劳动党第一书记、朝鲜国防委员会第一委员长金正恩为他们自安排的宴会。金正恩为少年团代表们送来牛肉、虹鳟鱼等，给予他们悉心的照顾。少年团代表们发誓，坚决只相信并跟随金正恩，准备成为强盛国家坚定的骨干。　（来源：中国新闻网）\n",
      "中新社北京６月２日电（记者　刘辰瑶）国资委２日在官网上回应了日前国家审计署发布的１５家中央企业２０１０年财务收支等审计结果公告，表示欢迎社会对央企进行监督。被“点名”的央企也于２日前纷纷对审计署提出的问题进行“检讨”，并称将进一步完善其制度。据悉，审计署１日公布了中石油、中石化、中国电科、一汽、二重等１５家中央企业２０１０年度财务收支等审计结果公告，披露了这些中央企业在财务管理、内部管理、投资项目管理等方面存在的问题。对于审计署指出的中国农业发展集团所属企业发车补贴两千余万元，中农发表示将对审计发现的问题边审边改，及时纠正。中石油集团也在官网发出声明，将结合２０１１年会计决算工作，完善相关制度和工作流程，限期整改并提交整改报告。中石化则表示要从源头上及时发现和堵塞漏洞。其他几家企业也在其官网做出了不同程度的“检讨”。据审计署企业审计司负责人表示，目前，已对８７名相关责任人进行了严肃处理，其中局级干部３人。对此，国资委指出，截至目前，相关企业修订、完善了２３３项规章制度，审计中发现的问题已有９７％完成了整改，其他问题正在积极整改中。国资委称，此次审计中发现了一些值得关注的问题，但从２０１２年审计署公布的审计评价意见看，中央企业的改革发展依旧取得了很大成绩，经营管理水平也有所提高。国资委还表示，将以此次审计报告发布为契机，督促相关企业认真落实审计整改意见，深入剖析原因，加强风险管控，堵塞管理漏洞，规范管理行为；各中央企业要紧密结合当前正在开展的“管理提升活动”，夯实基础管理，建立长效机制。\n",
      "证监会近日召开新闻通气会，就《关于加强与上市公司重大资产重组相关股票异常交易监管的暂行规定（征求意见稿）》向社会公开征求意见。根据该规定，上市公司停牌进入重大资产重组程序后，证券交易所将立即启动股票异常交易核查程序，并及时将股票异常交易信息上报证监会。证监会对股票异常交易信息进行核查后，如认为涉嫌内幕交易，决定立案稽查的，上市公司应暂停重组进程，并及时进行信息披露和风险提示。该规定明确，如果上市公司及其控股股东、实际控制人、占本次重组总交易额的比例在２０％以上的交易对方，因本次重大资产重组相关的内幕交易行为被证监会行政处罚或者被司法机关依法追究刑事责任的，证监会将终止审核。记者李会　（来源：经济日报）\n",
      "中国台湾网６月１５日消息　据台湾《中国时报》报道，岛内大学教授“假发票、真Ａ钱”案，如滚雪球，愈演愈烈，目前共有台北、台中、彰化、台南等四个地检署在侦办。最新一期的《时报周刊》报道，涉案教授可能多达１０００人，其中理工领域占三分之二强；由于台湾“检察总长”黄世铭定调以贪污罪嫌究办，涉案者将面临牢狱之灾，岛内各顶尖大学或学生，都会产生不小冲击。（中国台湾网　刘海伟）\n",
      "受到欧美股市大跌以及多部位联合吹风设立国际板的影响，今日沪深股市大幅跳空低开后持续震荡走低。截至收盘，上证指数报２３０８．５５点，跌幅２．７３％，深成指报９８７４．５２点，跌幅２．６７％。盘面看，今日重周期行业表现疲软，采掘、建材、机械设备等跌幅居前；相比而言，大消费类行业则相对抗跌，医药生物、食品饮料、餐饮旅游指数跌幅都在２％以内。行业对比可以看到投资者的心态极为谨慎，资金以避险品种为首选。两桶油及金融股全面下挫，是拖累指数最为重要的力量。中国石油收盘报９．３０元，创出２００９年１月１５日以来的收盘新低。就在不久之前，中石油大股东刚刚结束为期一年的增持计划，从目前看来，该增持计划并未对股价产生长期的积极影响。从上证指数今日的走势来看，早盘２６点的跳空缺口是去年８月１９日以来的最大向下跳空缺口。今年以来，由于整体市场环境趋暖，还没有出现不被回补的向下跳空缺口。不过此次跳空缺口处在相对高位，杀跌力度又比较大，短线看回补难度很大。今日两市成交额并未出现大幅放大，截至收盘沪市成交额８６９亿元，与沪综指全日近６４点的下跌幅度，以及击穿重要技术支撑位的走势并不相称，显示下方承接盘不多。事实上，５月中下旬至今上证指数一直徘徊于半年线附近，并多次受到支撑，此次再次考验此均线，支撑力度已经显著下降。尽管消息面上看欧美大跌以及国际板试点打击了市场信心，不过真正令投资者缺乏底气的恐怕还是对国内经济的诸多担忧，包括目前经济增速下滑时缺乏有效的政策手段对抗；过去担当经济发动机的部门目前产能过剩、结构转型艰难；０９年“四万亿”投资逐渐显现出后遗症等。在这种大背景下，沪深股市未来的上涨空间不大，不过考虑到目前市场整体估值较低，因此Ｌ型走势在所难免，考虑到指数目前处于年内的中位区，这意味着短线仍有一定下跌空间。　记者　申鹏　（来源：新华网）\n",
      "中新网６月１日电　据台湾“中央社”报道，台当局行政机构负责人陈冲今天（６月１日）到台湾立法机构进行施政方针报告并备询，不过由于“在野党”“立委”对开放含瘦肉精美牛进口、油电价双涨、证所税等政策不满，占据主席台，陈冲无法上台报告。马英九５月２０日再次任命陈冲为台当局“行政院长”，“立法院”依法邀请陈冲到“立法院”施政报告并备质询。不过由于陈冲没有接受“在野党”要求先为证所税造成的乱象道歉，也未承诺暂时冻涨电价，因此遭到杯葛（阻扰），让议事再次陷入停摆。民进党“立法院”党团干事长潘孟安说，陈冲今天到“立法院”做施政方针报告，应先为证所税版本朝令夕改道歉，并宣布１０日电　价缓涨，否则将杯葛议事，不让他上台。由于陈冲没有接受“在野党”要求，因此在他准备要上台报告时，“在野党”“立委”就一哄而上，占据发言台，阻挡陈冲报告，国民党籍“立委”也立刻在台下高喊，要“在野党”“停止内耗”、“不要作秀”。台“立法院长”王金平多次呼吁“在野党”“立委”回到座位无效后，只好宣布会议休息，并召集“朝野”协商。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#segment initial\n",
    "segment = [] #new list - 存储所有分完词的list \n",
    "for line in content:\n",
    "    try:\n",
    "        segs = jieba.lcut(line)  # lcut:listcut - segs is list\n",
    "        for seg in segs:\n",
    "            if len(seg)>1 and seg!='\\r\\n':  #新闻的文本>1非空 同时不等于换行 等赋值给segment \n",
    "                sgement.append(seg)\n",
    "    except:   #异常的话 print line \n",
    "        print(line) \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(['segment':segment])  #上面分完词后的结果\n",
    "\n",
    "# 读取stopwords 表\n",
    "stopwards = pd.read_csv(\"data/stopwards.txt\", index_col = False, quoting = 3,sep = \"\\t\",names = ['stopward'], encoding = 'utf-8')\n",
    "# stopwards.head()\n",
    "\n",
    "words_df = words_df[~words_df.segment.isin(stopwards.stopward)]  #分完词后的segment 词中，不再停用词当中的词取出来保留。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照sgement 来分组并通过聚合agg 来统计出的频次\n",
    "words_stat = words_df.groupby(by = ['segment'])['segment'].agg({\"计数\":numpy.size})\n",
    "\n",
    "# reset_index来拉平，并且是计数 这一列按照降序排序 ascending = False \n",
    "words_stat = words_stat.reset_index().sort_values(by = [\"计数\"],ascending = False)\n",
    "words_stat.head() # 从高到低显示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.画词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定字体 - simhei.ttf, background is \"white\" , max font size is 80\n",
    "wordcloud = WordCloud(font_path = \"data/simhei.ttf\",background_color = \"white\", max_font_size = 80)\n",
    "\n",
    "# 取出一部分高频出现的词语做可视化 - Dict \n",
    "# x[0] : word  - Key  \n",
    "# x[1] : frequency of word - Values \n",
    "word_frequency = {x[0]:x[1] for x in words_stat.head(1000).values}  # 取出前1000个词的values的数组构建一个字典\n",
    "wordcloud      = wordcloud.fit_words(word_frequency)\n",
    "# show the image\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ５.自定义背景图做词云的背景　－　白底"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib functions \n",
    "from scipy.misc import imread\n",
    "matplotlib.rcParams['figure.figsize'] = {15.0, 15.0}  # Fig size \n",
    "\n",
    "from wordcloud import WordCloud.ImageColorGenerator  # Color Generator \n",
    "# 读入图片作为模板\n",
    "bimg  = imread('image/sports.jpeg') \n",
    "\n",
    "# wordcloud 初始化: background, font and max font size ； mask 以上述图片作为模板； font_path  = 'data/simhei.ttf 微软雅黑字体\n",
    "wordcloud = WordCloud(background_color = 'white', mask = bimg, font_path  = 'data/simhei.ttf', max_font_size = 200 )\n",
    "word_frequency = {x[0]:x[1] for x in words_stat.head(1000).values}  # 取出前1000个词的values 的数组\n",
    "wordcloud      = wordcloud.fit_words(word_frequency)\n",
    "\n",
    "bimgColors     = ImageColorGenerator(bimg)  # 通过导入的图片来生成颜色\n",
    "plt.axis(\"off\")\n",
    "plt.show(wordcloud.recolor(color_func = bimgColors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (二) 中文自然语言处理文本分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.关键词提取  - based on TF - IDF: extract_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### based on TF-IDF \n",
    "## function : jieba.analyse.extract_tags(sentence, topK = 20, withWeight = False, allowPOS= ())\n",
    "## sentence is tested text \n",
    "## TopK 返回几个权重最大的几个关键词, default value  = 20\n",
    "## withWeight 决定是否返回关键词的权重， default value  = False \n",
    "## allowPOS 包含指定的词性； 名词或形容词，  default value  = NULL 即不筛选\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse as analyse \n",
    "import pandas as pd\n",
    "df  = pd.read_csv('C:\\Python_\\technologynews.csv',encoding = 'utf-8')\n",
    "df  = df.dropna()  # drop掉可能为空的行数\n",
    "lines   = df.content.values.tolist()  #取出数据中的content列，通过values tolist 转成Python列表\n",
    "\n",
    "# 利用join 函数将所有的文本拼接在一起\n",
    "content  = \"\".join(lines)\n",
    "\n",
    "# --\n",
    "# 按照空格打印出前30出现频率最高的词\n",
    "# 按照TF - IDF 计算值抽取出高频的词 - extract_tags()\n",
    "print \" \".join(analyse.extract_tags(content,topK = 30, withWeight = False, allowPOS = () ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.关键词提取 - based on textrank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## jieba.analyse.textrank(sentence,TopK = 20, withWeight = False, allowPOS = ('ns','n','vn','v')) # 默认是过滤词性的\n",
    "## \n",
    "## 1. 将待抽取关键词的文本进行分类\n",
    "## 2. 以固定窗口大小（default value = 5 ）, 词与词之间的共同出现的关系构建图\n",
    "## 3. 计算图中节点的pagerank\n",
    "## 4. 算法计算比TF-IDF 速度慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/sportsnews.csv\", encoding = 'utf-8')\n",
    "df = df.dropna()\n",
    "lines  = df.content.values.tolist()\n",
    "content  = \" \".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print - allowPOS 返回词性。默认是过滤词性  v: 动词, n:名词\n",
    "print \" \".join(analyse.textrank(sentence,TopK = 20, withWeight = False, allowPOS = ('v','n') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.LDA 主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###　无监督学习　－　抽取一定数量的主题，每个主题的权重从到大到小显示出来\n",
    "###　找到一堆文本中的主题，发现用户都在关心什么话题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1载入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwards \n",
    "stopwards = pd.read_csv(\"data/stopwards.txt\", index_col = False, quoting = 3,sep = \"\\t\", names = ['stopward'], encoding = 'utf-8')\n",
    "stopwards = stopwards['stopward'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化成合适的数据格式\n",
    "# 将文本内容处理成固定的格式，一个包含句子的List，每个list中的元素是分词后的词list；\n",
    "# [[第，一，条，新闻，在这里]， [第二条新闻在这里]， [这是在做什么，]]\n",
    "#\n",
    "import jieba.analyse as analyse \n",
    "import pandas as pd\n",
    "df  = pd.read_csv('C:\\Python_\\technologynews.csv',encoding = 'utf-8')\n",
    "df  = df.dropna()  # drop掉可能为空的行数\n",
    "lines   = df.content.values.tolist()  #取出数据中的content列，通过values tolist 转成Python列表\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        segs = jieba.lcut(line)  # jieba分词 - linecut \n",
    "        segs = filter(lamda x: len(x)>1, segs)  # 分词后词的数量比1小的是空，filter 掉不需要。\n",
    "        segs = filter(lamda x:x not in stopwards, segs)  # 分词完的词在stopwords中，同样不需要\n",
    "        sentences.append(segs)\n",
    "    except Exception,e:\n",
    "        print line \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check \n",
    "for word in sentences[4]:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bag of Words Model - 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words - 词袋模型\n",
    "\n",
    "dictonary = corpora.Dictionary(sentences) # 把 sentences 中的词建成字典(word ->index 映射)\n",
    "\n",
    "# 通过过词袋模型 将文本转化成数字, 学习下来新的词 - \n",
    "corpus    = [dictonary.doc2bow(sentence) for sentence in sentences]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LDA 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# topK = 20个主题 and corpus 表示设定好的格式数据\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictonary, num_topics = 20 )\n",
    "\n",
    "# Topk = 10 and 3nd 分类\n",
    "# 打印出第3种分类，及该分类中的前10个高频词及权重（权重由大到小显示）\n",
    "print lda.print_topic(3,topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印出主题，主题数目设定为20 ,每个主题里面的数量是8 - 可作为关键主题和信息的提取\n",
    "for topic in lad.print_topics(num_topics  = 20, num_words = 8):\n",
    "    print topic[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (三) 中文自然语言处理文本分类 by ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##　准备数据\n",
    "import jieba\n",
    "import pandas as pd \n",
    "\n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"C:\\Python_\\technologynews.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna() # 取消可能为空的行数\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"C:\\Python_\\carnews.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"C:\\Python_\\entertainmentnews.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# \n",
    "df_sprots   = pd.read_csv(\"C:\\Python_\\sportsnews.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个种类提取一定数量的文本样本记录\n",
    "technology  = df_technology.content.values.tolist()[100:2100]\n",
    "car  = df_car.content.values.tolist()[100:2100]\n",
    "\n",
    "entertainment  = df_entertainment.content.values.tolist()[100:2100]\n",
    "sprots  = df_sprotsdf_sprots.content.values.tolist()[100:2100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 1 records ramdomly\n",
    "print entertainment[11]\n",
    "\n",
    "print sprots[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 分词与中文文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load stopwards \n",
    "stopwards = pd.read_csv(\"data/stopwards.txt\", index_col = False, quoting = 3,sep = \"\\t\") # reading \n",
    "stopwards = stopwards['stopward'].values # load values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ML数据集的预处理 ： 去掉空格并打上标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords in fucntion \n",
    "def preprocess_text(content_lines,sentences,category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)  # 空格 清掉\n",
    "            segs = filter(lambda x:x not in stopwards, segs)\n",
    "             \n",
    "            # 把 sentences 里面的词通过空格链接起来，并打上category 的标签\n",
    "            sentences.append( (\" \".join(segs), category) )\n",
    "    except Exception, e:\n",
    "        print line\n",
    "        continue \n",
    "\n",
    "# generated date \n",
    "sentences =[]\n",
    "\n",
    "# 将全部的数据传进来做数据的预处理，并为各个种类带上标签： ‘technology’ and 'car' and so on \n",
    "preprocess_text(technology,sentences,'technology')\n",
    "preprocess_text(car,sentences,'car')\n",
    "preprocess_text(entertainment,sentences,'entertainment')\n",
    "preprocess_text(sprots,sentences,'sprots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 将数据分成测试数据集合和训练数据集之前，先提前打乱数据集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分成测试数据集合和训练数据集之前，先提前打乱数据集合\n",
    "import random\n",
    "random.shuffle(sentences)  # 乱序\n",
    "\n",
    "# print \n",
    "for sentence in sentences[:10]:\n",
    "    print sentence[0],sentence[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Split Data into Train and Test Data Serts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train and Test Data Serts \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 拉链函数来把数据的文本内容赋值给x, 标签  ‘technology’ and 'car' and so on  赋值给我label: y\n",
    "\n",
    "x,y  = zip(*sentences)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 200)\n",
    "\n",
    "# output \n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.通过词袋模型对文本进行特征提取，找到每个词对应的下标,并以下标作为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # 向量化计数器\n",
    "\n",
    "# 在上述通过空格衔接起来的词中取出4000个词，通过CountVectorizer 来找到每个词对应的词向量。该向量统计也会考虑每个词出现的频次\n",
    "vec  = CountVectorizer(analyzer = 'word', # 按照空格链接起来的词\n",
    "                      max_features = 4000) #  取出最高频的4000 个词，并以4000词的对应下标值组成一个向量\n",
    "\n",
    "# 用训练集做fit\n",
    "vec.fit(x_train)\n",
    "\n",
    "# 将字符串的np数组x（词语或者一句话）转化成向量\n",
    "def get_features(x):\n",
    "    x.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. import classifiter to fit data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 导入朴素贝叶斯分类器来训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import多项式的朴素贝叶斯模型作为分类器来训练模型\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# 将训练文本转化成向量的x_train 丢进来，及标签y_train 作为训练\n",
    "classifier.fit(vec.transform(x_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 计算准确率 on test data sets \n",
    "classifier.score(vec.transform(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## length of x_test data sets \n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 引入N-gram 模型做分词处理，N =1,2 : ngram_range = (1,3)，并加大词表数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # 向量化计数器\n",
    "\n",
    "vec  = CountVectorizer(analyzer = 'word', # 按照空格链接起来的词\n",
    "                      ngram_range = (1,3) # N-gram 模型 N = 1,2\n",
    "                      max_features = 20000) #  加大词表数量\n",
    "\n",
    "# 用训练集做fit\n",
    "vec.fit(x_train)\n",
    "\n",
    "# 将字符串的np数组x（词语）转化成向量\n",
    "def get_features(x):\n",
    "    x.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# 将训练文本转化成向量的x_train 丢进来，及标签y_train 作为训练\n",
    "classifier.fit(vec.transform(x_train,y_train))\n",
    "### 计算准确率 on test data sets \n",
    "classifier.score(vec.transform(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 通过交叉验证集合fit Model - crossvalidation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前数据为4个种类，这里KFold 成4个部分，三个部分做trian data sets, 1个部分做test data sets\n",
    "# 通过 KFold 来分类数据\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "# K 层抽烟的交叉验证\n",
    "def stratifiedkfold_cv(x,y,clf_class,shuffle= True,n_folds = 4,**kwargs):\n",
    "    stratifiedk_fold = StratifiedKFold(y,n_folds = n_folds, shuffle = shuffle) # 按照y 标签来类K 折\n",
    "    y_pred  = y[:]\n",
    "    \n",
    "    for train_index,test_index in stratifiedk_fold:\n",
    "        X_train, X_test =  x[train_index, test_index]\n",
    "        y_train         =  y[train_index]\n",
    "        clf  = clf_class(**kwargs)\n",
    "        clf.fit(X_trian,y_trian)\n",
    "        \n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "NB = MultinomialNB\n",
    "\n",
    "# print \n",
    "print precision_score(y,stratifiedkfold_cv(vec.transform(x), np.array(y),NB, shuffle = True, n_folds = 4, ))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (四) 整理文本分类Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.txet import CounterVectorizer \n",
    "from sklearn.model_selection import trian_test_split \n",
    "from sklearn.naive_bayes import MultiomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier():\n",
    "    \n",
    "    def __init__(self,classifier = MultiomialNB()) :  # 默认是朴素贝叶斯模型\n",
    "        self.classifier = classifier \n",
    "        self.vectorizer = CounterVectorizer(analyzer = 'word', ngram_range = (1,3), max_features = 2000)\n",
    "        \n",
    "    def features(self,X):\n",
    "        return self.vectorizer.transform(X)  # 将文本转化成向量\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        return self.classifier.score(self.features(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class的调用\n",
    "classifier  = TextClassifier()\n",
    "classifier.fit(x-train,y_train)\n",
    "\n",
    "# prediction & score \n",
    "print(classifier.predict('今 天 是 个 很 好 的 比 赛 时 间'))\n",
    "print(classifier.score(x_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (五) SVM Text Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(vec.transform(x_trian),  y_train)\n",
    "svm.score(vec.transform(x_test), y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Kernal - 高斯核 -速度比较慢\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC() # 默认RBF Kernal\n",
    "svm.fit(vec.transform(x_trian),  y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 换一个特征提取模型\n",
    "import re\n",
    "from sklearn.feature_extraction.text import SVM \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier():\n",
    "    \n",
    "    def __init__(self,classifier = SVC(kernel = 'linear')) :  # 默认是线性SVM\n",
    "        self.classifier = classifier \n",
    "        self.vectorizer = SVM(analyzer = 'word', ngram_range = (1,3), max_features = 2000)\n",
    "        \n",
    "    def features(self,X):\n",
    "        return self.vectorizer.transform(X)  # 将文本转化成向量\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        return self.classifier.score(self.features(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class的调用\n",
    "classifier  = TextClassifier()\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "# prediction & score \n",
    "print(classifier.predict('今 天 是 个 很 好 的 比 赛 时 间'))\n",
    "print(classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
