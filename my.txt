# Data processing and remove stopwards 
import pandas as pd 

# data set dict 
# df_technology
df_technology  = pd.read_csv("./data/technology_news.csv", encoding = 'utf-8')
df_technology  = df_technology.dropna()  # 空的字符drop 掉

# df_car
df_car  = pd.read_csv("./data/car_news.csv", encoding = 'utf-8')
df_car  = df_car.dropna()

# df_entertainment 
df_entertainment   = pd.read_csv("./data/entertainment_news.csv", encoding = 'utf-8')
df_entertainment   = df_entertainment.dropna()

# df_sports
df_sports   = pd.read_csv("./data/sports_news.csv", encoding = 'utf-8')
df_sports   = df_sports.dropna()

#df_military 
df_military  = pd.read_csv("./data/military_news.csv",encoding = 'utf-8')
df_military  = df_military.dropna()


# 提取出一定量的数据
# .values -> array 数组
# .tolist -> list 列表
# [1000:21000] -> 切片找出一部分的数据
technology = df_technology.content.values.tolist()[1000:21000]
car        = df_car.content.values.tolist()[1000:21000]
entertainment = df_entertainment.content.values.tolist()[1000:21000]
military   = df_military.content.values.tolist()[1000:21000]
sports     = df_sports.content.values.tolist()[1000:21000]


### remove stopwards
stopwords = pd.read_csv("data/stopwords_NLP.txt",index_col = False, quoting =3, sep= "\t", names =['stopwords'],encoding = 'utf-8' )
stopwords = stopwords['stopwords'].values
### Construct Data 
# Data Preparation 
import  jieba 
def preprocess_text_cnn(content_lines, sentences, category):
    for line in content_lines:
        try:
            segs = jieba.lcut(line)
            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉
            segs = filter(lambda x: x not in stopwords, segs)  # x in stopwards 过滤掉
            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on 
            sentences.append((" ".join(segs),category))  # 添加数据及其label
        except Exception as e:
            print (">>>",line)
            print ("Exception infor>>>",e)
            continue
import time 
sentences = []
start = time.time()
preprocess_text_cnn(technology,sentences,'technology')
preprocess_text_cnn(car,sentences,'car')
preprocess_text_cnn(entertainment,sentences,'entertainment')
preprocess_text_cnn(military,sentences,'military')
preprocess_text_cnn(sports,sentences,'sports')
end  = time.time()
print ("运行时间: %f(s)" %(end - start) )
"分割数据集"
import sklearn
"python3"
from  sklearn.cross_validation import train_test_split

"python2"
#from sklearn.model_selection import train_test_split 

# 拉链 将词语和label 分别分给x and y 
x,y = zip(*sentences)

# split the data into trianing and test data sets 
X_train_,X_test_,y_train,y_test   = train_test_split(x,y,random_state = 200)
# Variables Initialization
FLAGS = None
# 文档最长的长度
MAX_DOCUMENT_LENGTH = 100

# 最小词频数
MIN_WORD_FREQUENCY = 2 

# 词嵌入的维度
EMBEDDING_SIZE= 20

# filter 数量
N_FILTERS = 10

# Windows size 
WINDOWS_SIZE = 20

#filter 的形状
FILTER_SHAPE1 = [WINDOWS_SIZE, EMBEDDING_SIZE]
FILTER_SHAPE2 = [WINDOWS_SIZE, N_FILTERS]

# Pooling 
POOLING_WINDOW  = 4
POOLING_STRIDE  = 2 
n_words = 0
# define CNN Model 卷积神经网络
def cnn_model(features,target):
    ###
    ### 两层的卷积神经网络，用于短文本分类
    # 先把词转成词嵌入
    # 我们得到一个形状为[n_words,EMBEDDING_SIZE] 的词表映射矩阵
    # 接着我们可以把一批文本映射成[batch_size,sequence_length,EMBEDDING_SIZE]的矩阵
    
    # one - hot 编码 
    target = tf.one_hot(target,15,1,0)
    
    # 将feature/文本 的序列做一个映射，编成一个二维向量
    word_vectors = tf.contrib.layers.embed_sequence(features, vocab_size = n_words, embed_dim = EMBEDDING_SIZE,scope = 'words')
    
    # 将2维转成3 维
    word_vectors = tf.expand_dims(word_vectors,3)
    
    with tf.variable_scope('CNN_Layer1'):
        # 添加一个二维的卷积滤波
        conv1  = tf.contrib.layers.convolution2d(word_vectors,N_FILTERS,FILTER_SHAPE1,padding = 'VALID')
        # 添加RELU非线性 - 激活函数
        conv1  = tf.nn.relu(conv1)
        
        # maxmimum pooling 
        pool1 = tf.nn.max_pool(conv1,ksize = [1,POOLING_WINDOW,1,1], strides = [1,POOLING_STRIDE,1,1], padding = 'SAME')
        
        # 对矩阵转置 以满足形状
        pool1 = tf.transpose(pool1,[0,1,3,2])
        
    
    with tf.variable_scope('CNN_Layer2'):
            # 第2个卷积层
            conv2 = tf.contrib.layers.convolution2d(pool1,N_FILTERS,FILTER_SHAPE2,padding = 'VALID')
            
            # 抽取特征
            pool2  = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims = [1])
            
            
    # FullConnection - 全链接
    # 预测值 : logits
    logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn = None)  # 无激活函数
    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值， logits；预测值
    
    
    # 循环迭代
    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)
    
    # return
    return({
        'class': tf.argmax(logits,1),
        'prob': tf.nn.softmax(logits)
    }, loss, train_op)    
global n_words
# 处理词汇
vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)
X_train = np.array(list(vocab_processor.fit_transform(X_train_)))
X_test = np.array(list(vocab_processor.fit_transform(X_test_)))

n_words = len(vocab_processor.vocabulary_)
print('Total words: %d' %n_words)
#-------------
# 将类别映射成数字
import pandas 
cate_dic = {'technology':1,'car':2,'entertainment':3, 'military':4, 'sports':5}
y_train_map = map(lambda x:cate_dic[x],y_train)
y_test_map = map(lambda x:cate_dic[x],y_test)
y_train_list = list(y_train_map)
y_test_list  = list(y_test_map)
y_train_ser =  pandas.Series(y_train_list)
y_test_ser  =  pandas.Series(y_test_list)
# 构建模型
classifier = learn.SKCompat(learn.Estimator(model_fn = cnn_model))

# 训练和预测
classifier.fit(X_train,y_train_ser,steps = 1000)
y_predicted = classifier.predict(X_test)['class']

score = metrics.accuracy_score(y_test_ser,y_predicted)
print('Accuracy:{0:f}'.format(score))


# 使用RNN完成文本分类
# for python2 need to import lib
# from __future__ import absolute_import
# from __future__ import division
# from __future__ import print_function 

import argparse
import sys
import pandas as pd 
import numpy as np
from sklearn import metrics 
import tensorflow as tf 
from tensorflow.contrib.layers.python.layers import encoders 
learn  = tf.contrib.learn
FLAGS = None

# 通过词袋模型来一批一批的把数据灌进去
MAX_DOCUMENT_LENGTN = 15
MIN_WORD_FREQUENCE  = 1
EMBEDDING_SIZE      = 50

global n_words
# 处理词汇
#vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)
vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency=MIN_WORD_FREQUENCE)
x_train_ = np.array(list(vocab_processor.fit_transform(X_train_)))
x_test = np.array(list(vocab_processor.fit_transform(X_test_)))

n_words = len(vocab_processor.vocabulary_)
print('Total words: %d'%n_words)

def bag_of_words_model(feature,target):
    # 生成词袋模型
    target = tf.one_hot(target,15,1,0)
    features = encoders.bow_encoder(features,vocab_size = n_words,embed_dim = EMBEDDING_SIZE)

    # FullConnection - 全链接
    # 预测值 : logits
    logits = tf.contrib.layer.fully_connected(pool2, 15, activation_fn = None)  # 无激活函数
    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值， logits；预测值
    
    
    # 循环迭代
    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)
  
    # return
    return({
        'class': tf.argmax(logits,1),
        'prob': tf.nn.softmax(logits)
    }, loss, train_op)

#
model_fn = bag_of_words_model
classifier = learn.SKCompat(learn.Estimator(model_fn = model_fn))
classifier.fit(x_train_,y_train_ser,steps = 1000)
y_predicted = classifier.predict(x_test)['class']

score = metrics.accuracy_score(y_test,y_predicted)
print('Accuracy:{0:f}'.format(score))

