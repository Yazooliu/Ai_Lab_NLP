{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过Word2vec来处理文本 <<人民的名义>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"将以下文本名字加入文本内容，不再分词\"\n",
    "jieba.suggest_freq(\"李达康\",True)\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('田国富', True)\n",
    "jieba.suggest_freq('高育良', True)\n",
    "jieba.suggest_freq('侯亮平', True)\n",
    "jieba.suggest_freq('钟小艾', True)\n",
    "jieba.suggest_freq('陈岩石', True)\n",
    "jieba.suggest_freq('欧阳菁', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('蔡成功', True)\n",
    "jieba.suggest_freq('孙连城', True)\n",
    "jieba.suggest_freq('季昌明', True)\n",
    "jieba.suggest_freq('丁义珍', True)\n",
    "jieba.suggest_freq('郑西坡', True)\n",
    "jieba.suggest_freq('赵东来', True)\n",
    "jieba.suggest_freq('高小琴', True)\n",
    "jieba.suggest_freq('赵瑞龙', True)\n",
    "jieba.suggest_freq('林华华', True)\n",
    "jieba.suggest_freq('陆亦可', True)\n",
    "jieba.suggest_freq('刘新建', True)\n",
    "jieba.suggest_freq('刘庆祝', True)\n",
    "\n",
    "with open(\"./data/in_the_name_of_people.txt\") as f1:\n",
    "    read_document = f1.read()\n",
    "    #read_decode   = read_document.decode(\"GBK\")\n",
    "    read_lcut     = jieba.lcut(read_document)\n",
    "    read_join     = \" \".join(read_lcut)\n",
    "    read_result   = read_join.encode(\"utf-8\")\n",
    "    with open(\"./data/in_the_name_of_people_segment.txt\",'w') as f2:\n",
    "        f2.write(read_result) \n",
    "    \n",
    "f2.close()\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拿到Word2Vec 后的文件，一般在NLP的处理过程中，都要去停用词。由于word2vec依赖上下文，而且上写文就有可能是停词\n",
    "### 所以这里使用word2vec的时候不再去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"使用word2vec自带的LineSentence将文本加载到内存中\"\n",
    "import logging\n",
    "import os \n",
    "from gensim.models import word2vec \n",
    "common_texts = word2vec.LineSentence(\"./data/in_the_name_of_people_segment.txt\") \n",
    "model = word2vec.Word2Vec(common_texts, hs = 1,sg = 0,size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)\"找出跟省委书记沙瑞金下词向量相近的几个词\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "省委书记 沙瑞金 词向量接近的前top_count个词及其权重分别：\n",
      "高育良 权重: 0.958241820335\n",
      "易学习 权重: 0.942641675472\n",
      "李达康 权重: 0.941509485245\n",
      "检察长 权重: 0.933764100075\n",
      "季昌明 权重: 0.925242781639\n"
     ]
    }
   ],
   "source": [
    "print \"省委书记 沙瑞金 词向量接近的前top_count个词及其权重分别：\"\n",
    "top_count = 5\n",
    "for key in model.wv.similar_by_word(\"沙瑞金\".decode('utf-8'), topn =1000):\n",
    "        if len(key[0])%3 == 0: # 三个字的词\n",
    "            print key[0], \"权重:\", key[1]\n",
    "            top_count-=1\n",
    "            if top_count == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.word2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.word2vec\n",
      "\n",
      "FILE\n",
      "    c:\\python27\\lib\\site-packages\\gensim\\models\\word2vec.py\n",
      "\n",
      "DESCRIPTION\n",
      "    This module implements the word2vec family of algorithms, using highly optimized C routines,\n",
      "    data streaming and Pythonic interfaces.\n",
      "    \n",
      "    The word2vec algorithms include skip-gram and CBOW models, using either\n",
      "    hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations\n",
      "    in Vector Space <https://arxiv.org/pdf/1301.3781.pdf>`_, `Tomas Mikolov et al: Distributed Representations of Words\n",
      "    and Phrases and their Compositionality <https://arxiv.org/abs/1310.4546>`_.\n",
      "    \n",
      "    Other embeddings\n",
      "    ================\n",
      "    \n",
      "    There are more ways to train word vectors in Gensim than just Word2Vec.\n",
      "    See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText` and\n",
      "    wrappers for :class:`~gensim.models.wrappers.VarEmbed` and :class:`~gensim.models.wrappers.WordRank`.\n",
      "    \n",
      "    The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/\n",
      "    and extended with additional functionality and optimizations over the years.\n",
      "    \n",
      "    For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,\n",
      "    visit https://rare-technologies.com/word2vec-tutorial/.\n",
      "    \n",
      "    **Make sure you have a C compiler before installing Gensim, to use the optimized word2vec routines**\n",
      "    (70x speedup compared to plain NumPy implementation, https://rare-technologies.com/parallelizing-word2vec-in-python/).\n",
      "    \n",
      "    Usage examples\n",
      "    ==============\n",
      "    \n",
      "    Initialize a model with e.g.:\n",
      "    \n",
      "    >>> from gensim.test.utils import common_texts, get_tmpfile\n",
      "    >>> from gensim.models import Word2Vec\n",
      "    >>>\n",
      "    >>> path = get_tmpfile(\"word2vec.model\")\n",
      "    >>>\n",
      "    >>> model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
      "    >>> model.save(\"word2vec.model\")\n",
      "    \n",
      "    The training is streamed, meaning `sentences` can be a generator, reading input data\n",
      "    from disk on-the-fly, without loading the entire corpus into RAM.\n",
      "    \n",
      "    It also means you can continue training the model later:\n",
      "    \n",
      "    >>> model = Word2Vec.load(\"word2vec.model\")\n",
      "    >>> model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
      "    (0, 2)\n",
      "    \n",
      "    The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `model.wv`:\n",
      "    \n",
      "    >>> vector = model.wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    The reason for separating the trained vectors into `KeyedVectors` is that if you don't\n",
      "    need the full model state any more (don't need to continue training), the state can discarded,\n",
      "    resulting in a much smaller and faster object that can be mmapped for lightning\n",
      "    fast loading and sharing the vectors in RAM between processes::\n",
      "    \n",
      "    >>> from gensim.models import KeyedVectors\n",
      "    >>>\n",
      "    >>> path = get_tmpfile(\"wordvectors.kv\")\n",
      "    >>>\n",
      "    >>> model.wv.save(path)\n",
      "    >>> wv = KeyedVectors.load(\"model.wv\", mmap='r')\n",
      "    >>> vector = wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    Gensim can also load word vectors in the \"word2vec C format\", as a\n",
      "    :class:`~gensim.models.keyedvectors.KeyedVectors` instance::\n",
      "    \n",
      "    >>> from gensim.test.utils import datapath\n",
      "    >>>\n",
      "    >>> wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n",
      "    >>> wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)  # C binary format\n",
      "    \n",
      "    It is impossible to continue training the vectors loaded from the C format because the hidden weights,\n",
      "    vocabulary frequencies and the binary tree are missing. To continue training, you'll need the\n",
      "    full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,\n",
      "    not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.\n",
      "    \n",
      "    You can perform various NLP word tasks with a trained model. Some of them\n",
      "    are already built-in - you can see it in :mod:`gensim.models.keyedvectors`.\n",
      "    \n",
      "    If you're finished training a model (i.e. no more updates, only querying),\n",
      "    you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    >>> word_vectors = model.wv\n",
      "    >>> del model\n",
      "    \n",
      "    to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).\n",
      "    \n",
      "    Note that there is a :mod:`gensim.models.phrases` module which lets you automatically\n",
      "    detect phrases longer than one word. Using phrases, you can learn a word2vec model\n",
      "    where \"words\" are actually multiword expressions, such as `new_york_times` or `financial_crisis`:\n",
      "    \n",
      "    >>> from gensim.test.utils import common_texts\n",
      "    >>> from gensim.models import Phrases\n",
      "    >>>\n",
      "    >>> bigram_transformer = Phrases(common_texts)\n",
      "    >>> model = Word2Vec(bigram_transformer[common_texts], min_count=1)\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        BrownCorpus\n",
      "        LineSentence\n",
      "        PathLineSentences\n",
      "        Text8Corpus\n",
      "    gensim.models.base_any2vec.BaseWordEmbeddingsModel(gensim.models.base_any2vec.BaseAny2VecModel)\n",
      "        Word2Vec\n",
      "    gensim.utils.SaveLoad(__builtin__.object)\n",
      "        Word2VecTrainables\n",
      "        Word2VecVocab\n",
      "    \n",
      "    class BrownCorpus(__builtin__.object)\n",
      "     |  Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
      "     |  (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dirname)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LineSentence(__builtin__.object)\n",
      "     |  Iterate over a file that contains sentences: one line = one sentence.\n",
      "     |  Words must be already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      limit : int or None\n",
      "     |          Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from gensim.test.utils import datapath\n",
      "     |      >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
      "     |      >>> for sentence in sentences:\n",
      "     |      ...     pass\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PathLineSentences(__builtin__.object)\n",
      "     |  Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
      "     |  in alphabetical order by filename.\n",
      "     |  \n",
      "     |  The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
      "     |  .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
      "     |  \n",
      "     |  The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
      "     |  with words already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Warnings\n",
      "     |  --------\n",
      "     |  Does **not recurse** into subdirectories.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          Path to the directory.\n",
      "     |      limit : int or None\n",
      "     |          Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      iterate through the files\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Text8Corpus(__builtin__.object)\n",
      "     |  Iterate over sentences from the \"text8\" corpus, unzipped from http://mattmahoney.net/dc/text8.zip.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fname, max_sentence_length=10000)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      "     |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "     |  \n",
      "     |  Once you're finished training a model (=no more updates, only querying)\n",
      "     |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      "     |  \n",
      "     |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "     |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "     |  \n",
      "     |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "     |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "     |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "     |  \n",
      "     |  Some important attributes are the following:\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      "     |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |  \n",
      "     |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      "     |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      "     |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      "     |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      "     |  \n",
      "     |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      "     |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      "     |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      "     |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      "     |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2Vec\n",
      "     |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      "     |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(*args, **kwargs)\n",
      "     |      Deprecated. Use `self.wv.__contains__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      "     |  \n",
      "     |  __getitem__(*args, **kwargs)\n",
      "     |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      "     |  \n",
      "     |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of iterables, optional\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "     |          in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them).\n",
      "     |      size : int, optional\n",
      "     |          Dimensionality of the word vectors.\n",
      "     |      window : int, optional\n",
      "     |          Maximum distance between the current and predicted word within a sentence.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      sg : {0, 1}, optional\n",
      "     |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "     |      hs : {0, 1}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If set to 0, no negative sampling is used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      cbow_mean : {0, 1}, optional\n",
      "     |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "     |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      max_final_vocab : int, optional\n",
      "     |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "     |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "     |          Set to `None` if not required.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      hashfxn : function, optional\n",
      "     |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "     |      iter : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "     |          model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      sorted_vocab : {0, 1}, optional\n",
      "     |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      "     |      batch_words : int, optional\n",
      "     |          Target size (in words) for batches of examples passed to worker threads (and\n",
      "     |          thus cython routines).(Larger batches will be passed if individual\n",
      "     |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "     |      \n",
      "     |      >>> from gensim.models import Word2Vec\n",
      "     |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |      >>> model = Word2Vec(sentences, min_count=1)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the model's state.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      "     |          and learning rate.\n",
      "     |  \n",
      "     |  accuracy(*args, **kwargs)\n",
      "     |      Deprecated. Use `self.wv.accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      "     |  \n",
      "     |  clear_sims(self)\n",
      "     |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      "     |      \n",
      "     |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      "     |  \n",
      "     |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      "     |      Discard parameters that are used in training and scoring, to save memory.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      Use only if you're sure you're done training a model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace_word_vectors_with_normalized : bool, optional\n",
      "     |          If True, forget the original (not normalized) word vectors and only keep\n",
      "     |          the L2-normalized word vectors, to save even more memory.\n",
      "     |  \n",
      "     |  get_latest_training_loss(self)\n",
      "     |      Get current value of the training loss.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Current training loss.\n",
      "     |  \n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Deprecated. Use `self.wv.init_sims` instead.\n",
      "     |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      "     |  \n",
      "     |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      "     |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      "     |      where it intersects with the current vocabulary.\n",
      "     |      \n",
      "     |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      "     |      non-intersecting words are left alone.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          The file path to load the vectors from.\n",
      "     |      lockf : float, optional\n",
      "     |          Lock-factor value to be set for any imported word-vectors; the\n",
      "     |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      "     |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      "     |      binary : bool, optional\n",
      "     |          If True, `fname` is in the binary word2vec C format.\n",
      "     |      encoding : str, optional\n",
      "     |          Encoding of `text` for `unicode` function (python2 only).\n",
      "     |      unicode_errors : str, optional\n",
      "     |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      "     |  \n",
      "     |  predict_output_word(self, context_words_list, topn=10)\n",
      "     |      Get the probability distribution of the center word given context words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      context_words_list : list of str\n",
      "     |          List of context words.\n",
      "     |      topn : int, optional\n",
      "     |          Return `topn` words and their probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of (str, float)\n",
      "     |          `topn` length list of tuples of (word, probability).\n",
      "     |  \n",
      "     |  reset_from(self, other_model)\n",
      "     |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      "     |      \n",
      "     |      Structures copied are:\n",
      "     |          * Vocabulary\n",
      "     |          * Index to word mapping\n",
      "     |          * Cumulative frequency table (used for negative sampling)\n",
      "     |          * Cached corpus length\n",
      "     |      \n",
      "     |      Useful when testing multiple models on the same corpus in parallel.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Another model to copy the internal structures from.\n",
      "     |  \n",
      "     |  save(self, *args, **kwargs)\n",
      "     |      Save the model.\n",
      "     |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      "     |      online training and getting vectors for vocabulary words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the file.\n",
      "     |  \n",
      "     |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      "     |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      "     |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      "     |  \n",
      "     |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      "     |      Score the log probability for a sequence of sentences.\n",
      "     |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      "     |      \n",
      "     |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      "     |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      "     |      \n",
      "     |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      "     |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      "     |      \n",
      "     |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      "     |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      "     |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      "     |      how to use such scores in document classification.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |      total_sentences : int, optional\n",
      "     |          Count of sentences.\n",
      "     |      chunksize : int, optional\n",
      "     |          Chunksize of jobs\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |  \n",
      "     |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      "     |      Update the model's neural weights from a sequence of sentences.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "     |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      total_examples : int\n",
      "     |          Count of sentences.\n",
      "     |      total_words : int\n",
      "     |          Count of raw words in sentences.\n",
      "     |      epochs : int\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to`train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in sentences.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from gensim.models import Word2Vec\n",
      "     |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |      >>>\n",
      "     |      >>> model = Word2Vec(min_count=1)\n",
      "     |      >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "     |      >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      "     |      (1, 30)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(cls, *args, **kwargs) from __builtin__.type\n",
      "     |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      "     |          Save model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Loaded model.\n",
      "     |  \n",
      "     |  load_word2vec_format(cls, fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<type 'numpy.float32'>) from __builtin__.type\n",
      "     |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  log_accuracy(section)\n",
      "     |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `sentences` will be added to model's vocab.\n",
      "     |      progress_per : int, optional\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      **kwargs : object\n",
      "     |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      "     |  \n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          A mapping from a word in the vocabulary to its frequency count.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |  \n",
      "     |  doesnt_match(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.doesnt_match() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      "     |  \n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of unique tokens in the vocabulary\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int)\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |  \n",
      "     |  evaluate_word_pairs(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      "     |  \n",
      "     |  most_similar(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.most_similar() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      "     |  \n",
      "     |  most_similar_cosmul(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      "     |  \n",
      "     |  n_similarity(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.n_similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      "     |  \n",
      "     |  similar_by_vector(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      "     |  \n",
      "     |  similar_by_word(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.similar_by_word() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      "     |  \n",
      "     |  similarity(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      "     |  \n",
      "     |  wmdistance(*args, **kwargs)\n",
      "     |      Deprecated, use self.wv.wmdistance() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  cum_table\n",
      "     |  \n",
      "     |  hashfxn\n",
      "     |  \n",
      "     |  iter\n",
      "     |  \n",
      "     |  layer1_size\n",
      "     |  \n",
      "     |  min_count\n",
      "     |  \n",
      "     |  sample\n",
      "     |  \n",
      "     |  syn0_lockf\n",
      "     |  \n",
      "     |  syn1\n",
      "     |  \n",
      "     |  syn1neg\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "     |  Represents the inner shallow neural network used to train :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vector_size=100, seed=1, hashfxn=<built-in function hash>)\n",
      "     |  \n",
      "     |  prepare_weights(self, hs, negative, wv, update=False, vocabulary=None)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |  \n",
      "     |  reset_weights(self, hs, negative, wv)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |  \n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |      Get a random vector (but deterministic by seed_string).\n",
      "     |  \n",
      "     |  update_weights(self, hs, negative, wv)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(cls, fname, mmap=None) from __builtin__.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecVocab(gensim.utils.SaveLoad)\n",
      "     |  Vocabulary used by :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, max_final_vocab=None, ns_exponent=0.75)\n",
      "     |  \n",
      "     |  add_null_word(self, wv)\n",
      "     |  \n",
      "     |  create_binary_tree(self, wv)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  make_cum_table(self, wv, domain=2147483647L)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |      \n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |      \n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  prepare_vocab(self, hs, negative, wv, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |      \n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |      \n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |  \n",
      "     |  scan_vocab(self, sentences=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      "     |  \n",
      "     |  sort_vocab(self, wv)\n",
      "     |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(cls, fname, mmap=None) from __builtin__.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    array(...)\n",
      "        array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n",
      "        \n",
      "        Create an array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        object : array_like\n",
      "            An array, any object exposing the array interface, an object whose\n",
      "            __array__ method returns an array, or any (nested) sequence.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array.  If not given, then the type will\n",
      "            be determined as the minimum type required to hold the objects in the\n",
      "            sequence.  This argument can only be used to 'upcast' the array.  For\n",
      "            downcasting, use the .astype(t) method.\n",
      "        copy : bool, optional\n",
      "            If true (default), then the object is copied.  Otherwise, a copy will\n",
      "            only be made if __array__ returns a copy, if obj is a nested sequence,\n",
      "            or if a copy is needed to satisfy any of the other requirements\n",
      "            (`dtype`, `order`, etc.).\n",
      "        order : {'K', 'A', 'C', 'F'}, optional\n",
      "            Specify the memory layout of the array. If object is not an array, the\n",
      "            newly created array will be in C order (row major) unless 'F' is\n",
      "            specified, in which case it will be in Fortran order (column major).\n",
      "            If object is an array the following holds.\n",
      "        \n",
      "            ===== ========= ===================================================\n",
      "            order  no copy                     copy=True\n",
      "            ===== ========= ===================================================\n",
      "            'K'   unchanged F & C order preserved, otherwise most similar order\n",
      "            'A'   unchanged F order if input is F and not C, otherwise C order\n",
      "            'C'   C order   C order\n",
      "            'F'   F order   F order\n",
      "            ===== ========= ===================================================\n",
      "        \n",
      "            When ``copy=False`` and a copy is made for other reasons, the result is\n",
      "            the same as if ``copy=True``, with some exceptions for `A`, see the\n",
      "            Notes section. The default order is 'K'.\n",
      "        subok : bool, optional\n",
      "            If True, then sub-classes will be passed-through, otherwise\n",
      "            the returned array will be forced to be a base-class array (default).\n",
      "        ndmin : int, optional\n",
      "            Specifies the minimum number of dimensions that the resulting\n",
      "            array should have.  Ones will be pre-pended to the shape as\n",
      "            needed to meet this requirement.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            An array object satisfying the specified requirements.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones_like : Return an array of ones with shape and type of input.\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        full_like : Return a new array with shape of input filled with value.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When order is 'A' and `object` is an array in neither 'C' nor 'F' order,\n",
      "        and a copy is forced by a change in dtype, then the order of the result is\n",
      "        not necessarily 'C' as expected. This is likely a bug.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.array([1, 2, 3])\n",
      "        array([1, 2, 3])\n",
      "        \n",
      "        Upcasting:\n",
      "        \n",
      "        >>> np.array([1, 2, 3.0])\n",
      "        array([ 1.,  2.,  3.])\n",
      "        \n",
      "        More than one dimension:\n",
      "        \n",
      "        >>> np.array([[1, 2], [3, 4]])\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        Minimum dimensions 2:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], ndmin=2)\n",
      "        array([[1, 2, 3]])\n",
      "        \n",
      "        Type provided:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], dtype=complex)\n",
      "        array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
      "        \n",
      "        Data-type consisting of more than one element:\n",
      "        \n",
      "        >>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
      "        >>> x['a']\n",
      "        array([1, 3])\n",
      "        \n",
      "        Creating an array from sub-classes:\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'))\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
      "        matrix([[1, 2],\n",
      "                [3, 4]])\n",
      "    \n",
      "    default_timer = clock(...)\n",
      "        clock() -> floating point number\n",
      "        \n",
      "        Return the CPU time or real time since the start of the process or since\n",
      "        the first call to clock().  This has as much precision as the system\n",
      "        records.\n",
      "    \n",
      "    dot(...)\n",
      "        dot(a, b, out=None)\n",
      "        \n",
      "        Dot product of two arrays. Specifically,\n",
      "        \n",
      "        - If both `a` and `b` are 1-D arrays, it is inner product of vectors\n",
      "          (without complex conjugation).\n",
      "        \n",
      "        - If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
      "          but using :func:`matmul` or ``a @ b`` is preferred.\n",
      "        \n",
      "        - If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n",
      "          and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n",
      "        \n",
      "        - If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
      "          the last axis of `a` and `b`.\n",
      "        \n",
      "        - If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n",
      "          sum product over the last axis of `a` and the second-to-last axis of `b`::\n",
      "        \n",
      "            dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            First argument.\n",
      "        b : array_like\n",
      "            Second argument.\n",
      "        out : ndarray, optional\n",
      "            Output argument. This must have the exact kind that would be returned\n",
      "            if it was not used. In particular, it must have the right type, must be\n",
      "            C-contiguous, and its dtype must be the dtype that would be returned\n",
      "            for `dot(a,b)`. This is a performance feature. Therefore, if these\n",
      "            conditions are not met, an exception is raised, instead of attempting\n",
      "            to be flexible.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        output : ndarray\n",
      "            Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
      "            scalars or both 1-D arrays then a scalar is returned; otherwise\n",
      "            an array is returned.\n",
      "            If `out` is given, then it is returned.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the last dimension of `a` is not the same size as\n",
      "            the second-to-last dimension of `b`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        vdot : Complex-conjugating dot product.\n",
      "        tensordot : Sum products over arbitrary axes.\n",
      "        einsum : Einstein summation convention.\n",
      "        matmul : '@' operator as method with out parameter.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.dot(3, 4)\n",
      "        12\n",
      "        \n",
      "        Neither argument is complex-conjugated:\n",
      "        \n",
      "        >>> np.dot([2j, 3j], [2j, 3j])\n",
      "        (-13+0j)\n",
      "        \n",
      "        For 2-D arrays it is the matrix product:\n",
      "        \n",
      "        >>> a = [[1, 0], [0, 1]]\n",
      "        >>> b = [[4, 1], [2, 2]]\n",
      "        >>> np.dot(a, b)\n",
      "        array([[4, 1],\n",
      "               [2, 2]])\n",
      "        \n",
      "        >>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n",
      "        >>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n",
      "        >>> np.dot(a, b)[2,3,2,1,2,2]\n",
      "        499128\n",
      "        >>> sum(a[2,3,2,:] * b[1,2,:,2])\n",
      "        499128\n",
      "    \n",
      "    empty(...)\n",
      "        empty(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, without initializing entries.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of int\n",
      "            Shape of the empty array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            Desired output data-type for the array, e.g, `numpy.int8`. Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of uninitialized (arbitrary) data of the given shape, dtype, and\n",
      "            order.  Object arrays will be initialized to None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `empty`, unlike `zeros`, does not set the array values to zero,\n",
      "        and may therefore be marginally faster.  On the other hand, it requires\n",
      "        the user to manually set all the values in the array, and should be\n",
      "        used with caution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.empty([2, 2])\n",
      "        array([[ -9.74499359e+001,   6.69583040e-309],\n",
      "               [  2.13182611e-314,   3.06959433e-309]])         #random\n",
      "        \n",
      "        >>> np.empty([2, 2], dtype=int)\n",
      "        array([[-1073741821, -1067949133],\n",
      "               [  496041986,    19249760]])                     #random\n",
      "    \n",
      "    fromstring(...)\n",
      "        fromstring(string, dtype=float, count=-1, sep='')\n",
      "        \n",
      "        A new 1-D array initialized from text data in a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : str\n",
      "            A string containing the data.\n",
      "        dtype : data-type, optional\n",
      "            The data type of the array; default: float.  For binary input data,\n",
      "            the data must be in exactly this format.\n",
      "        count : int, optional\n",
      "            Read this number of `dtype` elements from the data.  If this is\n",
      "            negative (the default), the count will be determined from the\n",
      "            length of the data.\n",
      "        sep : str, optional\n",
      "            The string separating numbers in the data; extra whitespace between\n",
      "            elements is also ignored.\n",
      "        \n",
      "            .. deprecated:: 1.14\n",
      "                If this argument is not provided, `fromstring` falls back on the\n",
      "                behaviour of `frombuffer` after encoding unicode string inputs as\n",
      "                either utf-8 (python 3), or the default encoding (python 2).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        arr : ndarray\n",
      "            The constructed array.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the string is not the correct size to satisfy the requested\n",
      "            `dtype` and `count`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        frombuffer, fromfile, fromiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.fromstring('1 2', dtype=int, sep=' ')\n",
      "        array([1, 2])\n",
      "        >>> np.fromstring('1, 2', dtype=int, sep=',')\n",
      "        array([1, 2])\n",
      "    \n",
      "    score_cbow_pair(model, word, l1)\n",
      "        Score the trained CBOW model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        l1 : list of float\n",
      "            Vector representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    score_sentence_cbow(...)\n",
      "        score_sentence_cbow(model, sentence, _work, _neu1)\n",
      "        Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.cbow` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sentence_sg(...)\n",
      "        score_sentence_sg(model, sentence, _work)\n",
      "        Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.sg` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sg_pair(model, word, word2)\n",
      "        Score the trained Skip-gram model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        word2 : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    train_batch_cbow(...)\n",
      "        train_batch_cbow(model, sentences, alpha, _work, _neu1, compute_loss)\n",
      "        Update CBOW model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True, compute_loss=False, context_vectors=None, context_locks=None, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the CBOW algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        input_word_indices : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        l1 : list of float\n",
      "            Vector representation of the label word.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "    \n",
      "    train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "    \n",
      "    train_sg_pair(model, word, context_index, alpha, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None, compute_loss=False, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the Skip-gram algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        context_index : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    zeros(...)\n",
      "        zeros(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of ints\n",
      "            Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of zeros with the given shape, dtype, and order.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.zeros(5)\n",
      "        array([ 0.,  0.,  0.,  0.,  0.])\n",
      "        \n",
      "        >>> np.zeros((5,), dtype=int)\n",
      "        array([0, 0, 0, 0, 0])\n",
      "        \n",
      "        >>> np.zeros((2, 1))\n",
      "        array([[ 0.],\n",
      "               [ 0.]])\n",
      "        \n",
      "        >>> s = (2,2)\n",
      "        >>> np.zeros(s)\n",
      "        array([[ 0.,  0.],\n",
      "               [ 0.,  0.]])\n",
      "        \n",
      "        >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "        array([(0, 0), (0, 0)],\n",
      "              dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = -1\n",
      "    FAST_VERSION = 0\n",
      "    MAX_WORDS_IN_BATCH = 10000\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    exp = <ufunc 'exp'>\n",
      "    expit = <ufunc 'expit'>\n",
      "    log = <ufunc 'log'>\n",
      "    logaddexp = <ufunc 'logaddexp'>\n",
      "    logger = <logging.Logger object>\n",
      "    sqrt = <ufunc 'sqrt'>\n",
      "    string_types = (<type 'basestring'>,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）看看两个词向量的相识度，书中两个人物的相识度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9415095\n",
      "0.897839\n"
     ]
    }
   ],
   "source": [
    "print model.wv.similarity(\"沙瑞金\".decode(\"utf-8\"),\"李达康\".decode(\"utf-8\"))\n",
    "print model.wv.similarity(\"高小琴\".decode(\"utf-8\"),\"赵瑞龙\".decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)找出不在同一类的词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王大路\n"
     ]
    }
   ],
   "source": [
    "#help(model.wv)\n",
    "print model.wv.doesnt_match(u\"王大路 李达康 易学习 季昌明\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘新建\n"
     ]
    }
   ],
   "source": [
    "print model.wv.doesnt_match(u\"王大路 李达康 易学习 刘新建\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
