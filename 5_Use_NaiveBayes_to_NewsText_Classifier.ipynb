{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------\n",
    "# Using Naive_Bayes to do News Text Classifier \n",
    "# Modify History:\n",
    "#      2018- 12-9 \n",
    "#\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Modeling Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import os \n",
    "import time \n",
    "import random\n",
    "import jieba\n",
    "import sklearn \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np \n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.出掉stopwords中的重复性的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_deplicated_words(words):\n",
    "    words_set = set()  # 利用set 的属性仅存储单一的元素\n",
    "    with open(words,'r',encoding = 'utf-8') as readwords:\n",
    "        for line in readwords.readlines():\n",
    "            word  = line.strip()  # word is string \n",
    "            if len(word)>0 and word not in words_set: # 没有出现再现有的words 集合set 中则，增加进来\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.文本处理-用于训练及测试样本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Examples_Generated(folder_path,test_dataset_percentage = 0.20):\n",
    "    folder_list = os.listdir(folder_path)  # listdir - 用于列出该路劲下所有的文件及文件夹\n",
    "    data_list   = []\n",
    "    label_list  = []\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_list,folder)\n",
    "        files           = os.listdir(new_folder_path)  # 读取每个文件夹下面的全部文件\n",
    "        \n",
    "        # Reading Files\n",
    "        jindex =  1 # index initialization \n",
    "        for file in files:\n",
    "            # worry about memory if >100:\n",
    "            if jindex >= 100:\n",
    "                print(\"jindex >= 100,Break Now!\")\n",
    "            \n",
    "            # reading file from detail file path \n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding = 'utf-8') as open_handel:\n",
    "                read_rawdata = open_handel.read()\n",
    "            \n",
    "            # jieba cut - 分词\n",
    "            rawdata_cut      = jieba.cut(read_rewdata,cut_all = False)  # cut 精简model \n",
    "            rawdata_cut_list = list(rawdata_cut)\n",
    "            \n",
    "            data_list.append(rawdata_cut_list) # data \n",
    "            label_list.append(folder.decode('utf-8')) # folder name is the label of the data in this folder \n",
    "    \n",
    "    # divide the dataset into train and test data manual \n",
    "    data_label_list = zip(data_list,label_list) # 将两个list以元祖的形式组合成list = [(data_list, label_list)]\n",
    "    random.shuffle(data_label_list)  # 乱序\n",
    "    \n",
    "    # 手动区分 - Index_boundary \n",
    "    index_boundary = int(len(data_label_list)* test_dataset_percentage) \n",
    "    test_dataset   = data_label_list[0:index_boundary]\n",
    "    train_dataset  = data_label_list[index_boundary: ]\n",
    "    \n",
    "    # 通过zip(*) 将原来组合的list 再重写解开成两个单独的list, one is data, other one is list \n",
    "    [test_data,test_data_lable  ] = zip(*test_dataset) \n",
    "    [train_data,train_data_lable] = zip(*train_dataset)\n",
    "    \n",
    "    # using sklearn train_test_split to split train and test data\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #x_trian,x_test,y_train,y_test = train_test_split(train_dataset,test_dataset,random_state = 200)\n",
    "    \n",
    "    # 统计词频方法all_words_dict 中\n",
    "    all_words_dict  = {}\n",
    "    for words_list in train_data:\n",
    "        for words in words_list:\n",
    "            if all_words_dict.has_key(words):\n",
    "                all_words_dict[words ]+= 1 \n",
    "            else:\n",
    "                all_words_dict[words]  = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'has_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-500b394f78f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mall_words_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mall_words_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'has_key'"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    train_data = ['This ','is ','one','two','this']\n",
    "    all_words_dict  = {}\n",
    "    for words_list in train_data:\n",
    "        for word in words_list:\n",
    "            if all_words_dict.has_key(word):\n",
    "                all_words_dict[word]+= 1 \n",
    "            else:\n",
    "                all_words_dict[word]  = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_txtfile = './data/stopwords_cn_in5NavBayesTextClassifier.txt'\n",
    "stopwords_set     = remove_deplicated_words(stopwords_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1 = list(range(0,200,1))\n",
    "list_2 = list(range(0,int(len(list_1)* 0.2)))\n",
    "len(list_2)\n",
    "list_2[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
