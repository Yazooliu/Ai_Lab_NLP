{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然语言处理之 文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow need run under:\n",
      " 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#  Copyright private in 2018 \n",
    "#  Modify Date: \n",
    "#       2018 - 9 - 19\n",
    "#  Purpose : \n",
    "#       Text Analysise  by fasttext/word2vec/Deep learning/LSTM\n",
    "# ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import sklearn \n",
    "import sys \n",
    "print (\"TensorFlow need run under:\\n\",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hierarchical softmax - 类别较多时，通过构建哈夫曼编码来技术softmax layer 计算 和之前的word2vec 的trick\n",
    "# N-gram - 之使用unigram 的话会丢掉word order 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当文本量大时，可以通过fasttext来学习\n",
    "# data Exampe: \n",
    "#__label__2,.......content ......\n",
    "#__label__3,......content......\n",
    "#__label__4,......content......\n",
    "\n",
    "# Data Category \n",
    "# 1. car 2.sports 3.entertainment 4. technology 5. military"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (一). 通过Facebook 工业界fasttext 模型根据输入的新闻内容预测该新闻所属的种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督学习 - 新闻分类/或者用于用户情感的褒贬分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 生成文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import random \n",
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sprots\n",
    "df_sports   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sports   = df_sports.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"./data/military_news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n"
     ]
    }
   ],
   "source": [
    "print (technology[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　伴随全新途观L的上市，上汽大众大众品牌刷新了SUV产品谱系，构建起包含途观丝绸之路版、全新途观L及Teramont途昂在内的SUV产品矩阵，覆盖多个SUV细分市场，为消费者提供更为丰富的购车选择与更优质的汽车生活，助力上汽大众赢得更亮眼的市场表现。\n"
     ]
    }
   ],
   "source": [
    "print car[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　基于优酷强势内容，霸屏客厅的阿里家庭娱乐春节期间发出花式大招，精心编排，巧妙聚合，“春晚热播，明星扎堆，心疼爸妈，今年绝不熬着看”。花式看春晚第一弹是“明星组团儿上春晚”，囊括沈腾、贾玲、曹云金、小沈阳、冯巩、郭德纲、宋小宝、岳云鹏、青岛大姨、赵本山、蔡明、潘长江、郭冬临、黄宏、巩汉林共15位春晚笑匠，集合每个笑星历年春晚或综艺作品，将其精彩演出片段集结成辑打包放出。第二弹是以专题的形式聚合2017年春晚语言/歌曲/魔术戏曲三大类型，横向打穿，爱看小品的老爸老妈，追星的迷妹迷弟一键直达。\n"
     ]
    }
   ],
   "source": [
    "print entertainment[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　报道称，美国一些防务专家批评白宫错失了一些适当采取军事手段但不至于引发战争的机会。\n"
     ]
    }
   ],
   "source": [
    "print military[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　,中新网,清远1月20日电 (记者 唐贵江)2017/18“叮咚出行”广东超级杯七人制足球联赛(清远杯)暨2017青联杯足球联赛启动仪式，20日在清远清新体育馆足球场举行，中国足球协会副主席、广东省足球协会名誉主席、广东省民间足球促进会会长容志行为赛事授牌，并勉励活跃群众足球运动。\n"
     ]
    }
   ],
   "source": [
    "print sports[82]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stopwords_read  = pd.read_csv(\"./data/stopwords_NLP.txt\", index_col = False, quoting = 3, sep = \"\\t\", names = ['stopword'], encoding = 'utf-8')\n",
    "stopwords  = stopwords_read['stopword'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+' ',' '-' '--' '.' '..' '...' '......' '...................' './' '.一'\n",
      " '记者' '数' '年' '月' '日' '时' '分' '秒' '/' '//' '0' '1' '2' '3' '4' '5' '6' '7'\n",
      " '8' '9' ':' '://' '::' ';' '<' '=' '>' '>>' '?' '@' 'A' 'Lex' '[' '\\\\' ']'\n",
      " '【' '】' '^' '_' '`' 'exp' 'sub' 'sup' '|' '}' '~' '~~~~' '·' '×' '×××' 'Δ'\n",
      " 'Ψ' 'γ' 'μ' 'φ' 'φ．' 'В' '—' '——' '———' '‘' '’' '’‘' '“' '”' '”，' '…' '……'\n",
      " '…………………………………………………③' '′∈' '′｜' '℃' 'Ⅲ' '↑' '→' '∈［' '∪φ∈' '≈' '①' '②'\n",
      " '②ｃ' '③' '③］' '④' '⑤' '⑥' '⑦' '⑧' '⑨' '⑩' '──']\n"
     ]
    }
   ],
   "source": [
    "print (stopwords[10:112])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Text Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'technology' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-770534760052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# preprocess the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtechnology\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'technology'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'car'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentertainment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcate_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entertainment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'technology' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category,stopwords):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwords, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\"__label__\"+str(category)+\", \"+\" \" .join(segs))\n",
    "        except Exception as e:\n",
    "            print (\"Exception infor\",e)\n",
    "            #print line\n",
    "            continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences,cate_dic['technology'],stopwords)\n",
    "preprocess_text(car,sentences,cate_dic['car'],stopwords)\n",
    "preprocess_text(entertainment,sentences,cate_dic['entertainment'],stopwords)\n",
    "preprocess_text(military,sentences,cate_dic['military'],stopwords)\n",
    "preprocess_text(sports,sentences,cate_dic['sports'],stopwords)\n",
    "\n",
    "# 乱序处理 - 使得同一类别的样本不扎堆出现在一起\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.写入文本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format\n",
      "Done....\n"
     ]
    }
   ],
   "source": [
    "print (\"writing data to fasttext format\")\n",
    "openout  = open('training_datasets.txt','w')  # 写入的方式打开\n",
    "\n",
    "for sentence in sentences:\n",
    "    openout.write(sentence.encode('utf-8') + \"\\n\")  # 中文形式  encode('utf-8') + \"\\n\" 换行符\n",
    "print (\"Done....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.调用fasttext 训练生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a437ab576316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_datasets.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'classifier.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_prefix\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'__label__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "classifier  = fasttext.supervised('training_datasets.txt','classifier.model', label_prefix  = '__label__') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test and evaluted the data set \n",
    "testresult = classifier.test('training_datasets.txt')\n",
    "\n",
    "# printing 准确值和召回率\n",
    "print 'testresult precision', testresult.precision \n",
    "print 'testresult recall', testresult.recall \n",
    "\n",
    "print 'Number of examples: ', testresult.nexamples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.实际预测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_cata_test = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# 待测试样本\n",
    "texts  = ['马来西亚 反贪 委员会 称 马来西亚 前总理 纳吉布 被逮捕 因其牵涉']\n",
    "labels = classifier.predict(texts)\n",
    "\n",
    "#printing label and category \n",
    "print('labels is :', labels)\n",
    "print label_to_cata_test[int(labels[0][0])]\n",
    "\n",
    "\n",
    "# 同时输出有多少的概率来肯定种类是这个\n",
    "labels = classifier.predict_proba(texts)\n",
    "print labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TopK 预测结果分析\n",
    "# K = 5\n",
    "category = classifier.predict(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输出category 及其对应的概率\n",
    "category = classifier.predict_proba(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (二). 通过fasttext做无监督文本学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-22-42f8b5b0be49>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-42f8b5b0be49>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    print \"writing data to  fasttext unsupervised learning format ...\"\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "        except Exception as e:\n",
    "            print (\">>>\",e)\n",
    "            continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences)\n",
    "preprocess_text(car,sentences)\n",
    "preprocess_text(entertainment,sentences)\n",
    "preprocess_text(military,sentences)\n",
    "preprocess_text(sports,sentences)\n",
    "\n",
    "\n",
    "# print out \n",
    "print \"writing data to  fasttext unsupervised learning format ...\"\n",
    "writeout = open('unsupervised_trainingdatasets.txt','w')\n",
    "\n",
    "for sentence in sentences: \n",
    "    writeout.write(sentence.encode('utf-8')+ \"\\n\")\n",
    "    \n",
    "print\"write Done ...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using fasttext to training the data sets\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('unsupervised_trainingdatasets.txt','model')\n",
    "# print the list \n",
    "print model.words\n",
    "\n",
    "# CBOW model - continue bags of words \n",
    "model  = fasttext.cbow('unsupervised_trainingdatasets.txt','model')\n",
    "print model.words # list of words from dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (三). Gensim vs Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成无监督训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data - 无监督不需要标签\n",
    "preprocess_text(technology,sentences)\n",
    "preprocess_text(car,sentences)\n",
    "preprocess_text(entertainment,sentences)\n",
    "preprocess_text(military,sentences)\n",
    "preprocess_text(sports,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##model fitting \n",
    "model = Word2Vec(sentences, size = 100, window = 5, min_count = 5, workers = 4)\n",
    "model.save(gensim_word2vec.model)\n",
    "model.wv['信息']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (四).文本分类by Deep Learning  - 有监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: \n",
    "#并不是将全部数据全部加在到内容，而是将一个batch 一个batch 学习及权重更新去学习新的模型，以克服内存不够的问题\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used on short text processing \n",
    "# LSTM can be used to long text processing\n",
    "\n",
    "# CNN 中的filter 窗口大小跟词向量的文本大小有关，每个词的窗口可能为 词向量的个数* 每个词向量的维数\n",
    "# 窗口filter + pooling 池化 + fullconnection 全链接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing and remove stopwards \n",
    "import pandas as pd \n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sports\n",
    "df_sports   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sports   = df_sports.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"./data/military_news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n",
    "\n",
    "\n",
    "### remove stopwards\n",
    "stopwords = pd.read_csv(\"data/stopwords_NLP.txt\",index_col = False, quoting =3, sep= \"\\t\", names =['stopwords'],encoding = 'utf-8' )\n",
    "stopwords = stopwords['stopwords'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Construct Data \n",
    "# Data Preparation \n",
    "import  jieba \n",
    "def preprocess_text_cnn(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwords, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append((\" \".join(segs),category))  # 添加数据及其label\n",
    "        except Exception as e:\n",
    "            print (\">>>\",line)\n",
    "            print (\"Exception infor>>>\",e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Yazhou\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.578 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 6.000000(min)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "sentences = []\n",
    "start = time.time()\n",
    "preprocess_text_cnn(technology,sentences,'technology')\n",
    "preprocess_text_cnn(car,sentences,'car')\n",
    "preprocess_text_cnn(entertainment,sentences,'entertainment')\n",
    "preprocess_text_cnn(military,sentences,'military')\n",
    "preprocess_text_cnn(sports,sentences,'sports')\n",
    "end  = time.time()\n",
    "print (\"运行时间: %f(min)\" %(int((end - start)/60) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始样本数据长度是: 86595\n"
     ]
    }
   ],
   "source": [
    "print (\"初始样本数据长度是:\",len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"分割数据集\"\n",
    "import sklearn\n",
    "\"python3\"\n",
    "from  sklearn.cross_validation import train_test_split\n",
    "\n",
    "\"python2\"\n",
    "#from sklearn.model_selection import train_test_split \n",
    "\n",
    "# 拉链 将词语和label 分别分给x and y \n",
    "x,y = zip(*sentences)\n",
    "\n",
    "# split the data into trianing and test data sets \n",
    "X_train_,X_test_,y_train,y_test   = train_test_split(x,y,random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过CNN(短文本局部信息还是挺有用的)和RNN-LSTM处理(长文本)的文本分类问题，使用浅层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "import tensorflow as tf \n",
    "learn  = tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables Initialization\n",
    "FLAGS = None\n",
    "# 文档最长的长度\n",
    "MAX_DOCUMENT_LENGTH = 100  # 没有达到长度补零\n",
    "\n",
    "# 最小词频数\n",
    "MIN_WORD_FREQUENCY = 2   # 统计至少出现两次的词\n",
    "\n",
    "# 词嵌入的维度\n",
    "EMBEDDING_SIZE= 20  # 数据样本变大的话，词向量的维度也要加大，这回影响CPU的计算复杂度  ,词向量的维度\n",
    "\n",
    "# filter 数量\n",
    "N_FILTERS = 10  # \n",
    "\n",
    "# Windows size \n",
    "WINDOWS_SIZE = 20  # 一次取多少个词,滑动窗口的大小是: WINDOWS_SIZE * EMBEDDING_SIZE\n",
    "\n",
    "#filter 的形状\n",
    "FILTER_SHAPE1 = [WINDOWS_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOWS_SIZE, N_FILTERS]\n",
    "\n",
    "# Pooling \n",
    "POOLING_WINDOW  = 4  # 窗口大小\n",
    "POOLING_STRIDE  = 2   # 窗口滑动的步长\n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 利用卷积神经网络CNN做短文本的信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define CNN Model 卷积神经网络\n",
    "def cnn_model(features,target):\n",
    "    ###\n",
    "    ### 两层的卷积神经网络，用于短文本分类\n",
    "    # 先把词转成词嵌入\n",
    "    # 我们得到一个形状为[n_words,EMBEDDING_SIZE] 的词表映射矩阵\n",
    "    # 接着我们可以把一批文本映射成[batch_size,sequence_length,EMBEDDING_SIZE]的矩阵\n",
    "    \n",
    "    # one - hot 编码 \n",
    "    target = tf.one_hot(target,25,1,0) # 类别的编码，可以是5 维，这里是15 ,6 后面的可以补零\n",
    "    \n",
    "    # 将feature/文本 的序列做一个映射embed_sequence，编成一个二维向量，vocab_size 词表大小，embed_dim 每个词映射成词向量的维度20维\n",
    "    # 对文本的序列映射成词嵌入\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(features, vocab_size = n_words, embed_dim = EMBEDDING_SIZE,scope = 'words')\n",
    "    \n",
    "    # 将2维增加成转成3 维，第3维为1\n",
    "    word_vectors = tf.expand_dims(word_vectors,3) # 例如维度是2*5*1 ，最后一层是1\n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        # 添加一个二维的卷积滤波  # N_FILTERS ， FILTER_SHAPE1 窗口大小；padding 补零\n",
    "        conv1  = tf.contrib.layers.convolution2d(word_vectors,N_FILTERS,FILTER_SHAPE1,padding = 'VALID') # word_vectors 做好词嵌入编码的词向量\n",
    "        # 添加RELU非线性 - 激活函数\n",
    "        conv1  = tf.nn.relu(conv1)\n",
    "        \n",
    "        # maxmimum pooling  # POOLING_WINDOW 窗口大小，POOLING_STRIDE 滑动步长\n",
    "        pool1 = tf.nn.max_pool(conv1,ksize = [1,POOLING_WINDOW,1,1], strides = [1,POOLING_STRIDE,1,1], padding = 'SAME')\n",
    "        \n",
    "        # 对矩阵转置 以满足形状\n",
    "        pool1 = tf.transpose(pool1,[0,1,3,2])\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer2'):\n",
    "            # 第2个卷积层  # N_FILTERS 神经元个数\n",
    "            conv2 = tf.contrib.layers.convolution2d(pool1,N_FILTERS,FILTER_SHAPE2,padding = 'VALID')\n",
    "            \n",
    "            # 抽取特征 - 压缩类似max pooling\n",
    "            pool2  = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims = [1])\n",
    "            \n",
    "            \n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    \"softmax_cross_entropy - 基于softmax 的交叉熵损失函数\" \n",
    "    logits = tf.contrib.layers.fully_connected(pool2, 25, activation_fn = None)  # 15： 列别：无激活函数\n",
    "    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值标准答案， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "    \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow.preprocessing 里包含的VocabularyProcessor（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp  = ['I am good', 'You are  here ','I am glad', 'it is great']\n",
    "#\n",
    "# 只要出现的最小频率是1 或者比1 大，就处理\n",
    "\"以max_document_length=10，最小词频为1 的形式对上述的list做编码，没有都编码成长度为10的向量\"\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(10, min_frequency=1)  \n",
    "list(vocab_processor.fit_transform(temp))\n",
    "\n",
    "# I am good -> [1,2,0,0,......] length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VocabularyProcessor in module tensorflow.contrib.learn.python.learn.preprocessing.text:\n",
      "\n",
      "class VocabularyProcessor(builtins.object)\n",
      " |  Maps documents to sequences of word ids.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_document_length, min_frequency=0, vocabulary=None, tokenizer_fn=None)\n",
      " |      Initializes a VocabularyProcessor instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        max_document_length: Maximum length of documents.\n",
      " |          if documents are longer, they will be trimmed, if shorter - padded.\n",
      " |        min_frequency: Minimum frequency of words in the vocabulary.\n",
      " |        vocabulary: CategoricalVocabulary object.\n",
      " |      \n",
      " |      Attributes:\n",
      " |        vocabulary_: CategoricalVocabulary object.\n",
      " |  \n",
      " |  fit(self, raw_documents, unused_y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Args:\n",
      " |        raw_documents: An iterable which yield either str or unicode.\n",
      " |        unused_y: to match fit format signature of estimators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, unused_y=None)\n",
      " |      Learn the vocabulary dictionary and return indexies of words.\n",
      " |      \n",
      " |      Args:\n",
      " |        raw_documents: An iterable which yield either str or unicode.\n",
      " |        unused_y: to match fit_transform signature of estimators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
      " |  \n",
      " |  reverse(self, documents)\n",
      " |      Reverses output of vocabulary mapping to words.\n",
      " |      \n",
      " |      Args:\n",
      " |        documents: iterable, list of class ids.\n",
      " |      \n",
      " |      Yields:\n",
      " |        Iterator over mapped in words documents.\n",
      " |  \n",
      " |  save(self, filename)\n",
      " |      Saves vocabulary processor into given file.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: Path to output file.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to word-id matrix.\n",
      " |      \n",
      " |      Convert words to ids with vocabulary fitted with fit or the one\n",
      " |      provided in the constructor.\n",
      " |      \n",
      " |      Args:\n",
      " |        raw_documents: An iterable which yield either str or unicode.\n",
      " |      \n",
      " |      Yields:\n",
      " |        x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  restore(filename) from builtins.type\n",
      " |      Restores vocabulary processor from given file.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: Path to file to load from.\n",
      " |      \n",
      " |      Returns:\n",
      " |        VocabularyProcessor object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(learn.preprocessing.VocabularyProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 67311\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "# 处理词汇\n",
    "\"将文本编码成最长为100，最小频次为1 的词向量\"\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)\n",
    "\n",
    "X_train = np.array(list(vocab_processor.fit_transform(X_train_)))\n",
    "X_test = np.array(list(vocab_processor.transform(X_test_)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' %n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "# 将类别映射成数字\n",
    "import pandas \n",
    "cate_dic = {'technology':1,'car':2,'entertainment':3, 'military':4, 'sports':5}\n",
    "y_train_map = map(lambda x:cate_dic[x],y_train)\n",
    "y_test_map = map(lambda x:cate_dic[x],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ser =  pandas.Series(y_train_map)\n",
    "y_test_ser  =  pandas.Series(y_test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_map>>>  <map object at 0x000000001E30C2B0>\n",
      "xy_map>>>  <map object at 0x000000001E30C2B0>\n",
      "xy_map>>>  <map object at 0x000000001E30C2B0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n('output>>> ', [4, 10, 18])\\n('output>>> ', [4, 10, 18])\\n('output>>> ', [4, 10, 18])\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_map = map(lambda x,y:x*y,[1,2,3],[4,5,6])\n",
    "for i in xy_map:\n",
    "    print (\"xy_map>>> \",xy_map)### Python3中，这个是迭代器，不会打印出结果\n",
    "\n",
    "\"下面是在python2中运行的结果：\"\n",
    "\"\"\"\n",
    "('output>>> ', [4, 10, 18])\n",
    "('output>>> ', [4, 10, 18])\n",
    "('output>>> ', [4, 10, 18])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"卷积神经网络构建模型\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001E631128>, '_evaluation_master': '', '_model_dir': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_master': '', '_keep_checkpoint_max': 5, '_task_id': 0, '_task_type': None, '_tf_random_seed': None, '_environment': 'local'}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpvkefl40j\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpvkefl40j\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2187958, step = 1\n",
      "INFO:tensorflow:global_step/sec: 9.60007\n",
      "INFO:tensorflow:loss = 0.6099452, step = 101 (10.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.63986\n",
      "INFO:tensorflow:loss = 0.4405744, step = 201 (10.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.37241\n",
      "INFO:tensorflow:loss = 0.40325755, step = 301 (10.670 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.49974\n",
      "INFO:tensorflow:loss = 0.39999416, step = 401 (10.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.62501\n",
      "INFO:tensorflow:loss = 0.2680375, step = 501 (10.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.56334\n",
      "INFO:tensorflow:loss = 0.21278459, step = 601 (10.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.57066\n",
      "INFO:tensorflow:loss = 0.1727003, step = 701 (10.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.49703\n",
      "INFO:tensorflow:loss = 0.21250173, step = 801 (10.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.59454\n",
      "INFO:tensorflow:loss = 0.28521088, step = 901 (10.423 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpvkefl40j\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.18629113.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpvkefl40j\\model.ckpt-1000\n",
      "通过卷积神经CNN网络获得的准确率是:0.883828\n"
     ]
    }
   ],
   "source": [
    "classifier = learn.SKCompat(learn.Estimator(model_fn = cnn_model))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(X_train,y_train_ser,steps = 1000) # steps 步长\n",
    "y_predicted = classifier.predict(X_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test_ser,y_predicted)\n",
    "print('通过卷积神经CNN网络获得的准确率是:{0:f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 使用RNN：LSTM -词袋模型来完成长文本信息文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'可以利用LSTM的捕获时序信息的特产，作用于长信息文本'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"可以利用RNN-LSTM的捕获时序信息的特产，作用于长信息文本\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用RNN完成文本分类 ： Cell 底层已经做好了\n",
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "import tensorflow as tf \n",
    "from tensorflow.contrib.layers.python.layers import encoders # 词袋模型编码器\n",
    "learn  = tf.contrib.learn\n",
    "FLAGS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 67311\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001EEA87B8>, '_evaluation_master': '', '_model_dir': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_master': '', '_keep_checkpoint_max': 5, '_task_id': 0, '_task_type': None, '_tf_random_seed': None, '_environment': 'local'}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmppdg1yfcy\n",
      "WARNING:tensorflow:From <ipython-input-18-a69f870de7a3>:24: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmppdg1yfcy\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7084594, step = 1\n",
      "INFO:tensorflow:global_step/sec: 15.3295\n",
      "INFO:tensorflow:loss = 0.45670223, step = 101 (6.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.5536\n",
      "INFO:tensorflow:loss = 0.41660628, step = 201 (6.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.5398\n",
      "INFO:tensorflow:loss = 0.39517123, step = 301 (5.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.3436\n",
      "INFO:tensorflow:loss = 0.32025522, step = 401 (6.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.5581\n",
      "INFO:tensorflow:loss = 0.14036131, step = 501 (6.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.9201\n",
      "INFO:tensorflow:loss = 0.14162683, step = 601 (5.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.9277\n",
      "INFO:tensorflow:loss = 0.11445075, step = 701 (6.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.3131\n",
      "INFO:tensorflow:loss = 0.16388269, step = 801 (6.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9113\n",
      "INFO:tensorflow:loss = 0.12650897, step = 901 (7.189 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmppdg1yfcy\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.18646052.\n",
      "WARNING:tensorflow:From <ipython-input-18-a69f870de7a3>:24: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmppdg1yfcy\\model.ckpt-1000\n",
      "通过词袋模型得到的准确率是:0.909834\n"
     ]
    }
   ],
   "source": [
    "# 通过词袋模型来一批一批的把数据灌进去\n",
    "MAX_DOCUMENT_LENGTN = 15\n",
    "MIN_WORD_FREQUENCE  = 1\n",
    "EMBEDDING_SIZE      = 50\n",
    "\n",
    "global n_words\n",
    "# 处理词汇 文本映射成 词袋\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_train_)))\n",
    "x_test = np.array(list(vocab_processor.transform(X_test_)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d'%n_words)\n",
    "\n",
    "def bag_of_words_model(features,target):\n",
    "    # 生成词袋模型\n",
    "    target = tf.one_hot(target,15,1,0) # 15-》5\n",
    "    features = encoders.bow_encoder(features,vocab_size = n_words,embed_dim = EMBEDDING_SIZE)\n",
    "\n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    logits = tf.contrib.layers.fully_connected(features, 15, activation_fn = None)  # 无激活函数\n",
    "    \"基于softmax 的交叉熵损失\"\n",
    "    loss   = tf.contrib.losses.softmax_cross_entropy(logits, target)  #target:真实值， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代  # learning_rate 因优化器而已，查官方论文看看每个模型的建议学习率\n",
    "    \"迭代以最小化loss 函数\"\n",
    "    \"学习率高了会震荡，小了学习花费时间\"\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framework.get_global_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "  \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n",
    "\n",
    "#\n",
    "\"词袋模型，一个batch一个batch 灌入\"\n",
    "model_fn = bag_of_words_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn = model_fn))\n",
    "# 训练和预测\n",
    "classifier.fit(x_train,y_train_ser,steps = 1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test_ser,y_predicted)\n",
    "print('通过词袋模型得到的准确率是:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### 使用RNN（这里使用的是GRU）来完成文本的分类工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000000020695080>, '_is_chief': True, '_num_worker_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_environment': 'local', '_task_id': 0, '_master': '', '_save_checkpoints_steps': None, '_task_type': None, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_keep_checkpoint_max': 5}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpkh1ba3b8\n",
      "WARNING:tensorflow:From <ipython-input-38-be3c93f696ef>:26: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpkh1ba3b8\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7051415, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.63408\n",
      "INFO:tensorflow:loss = 0.6334941, step = 101 (21.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.4174\n",
      "INFO:tensorflow:loss = 0.35489213, step = 201 (18.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.95291\n",
      "INFO:tensorflow:loss = 0.2940426, step = 301 (20.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.26258\n",
      "INFO:tensorflow:loss = 0.22473845, step = 401 (19.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.30981\n",
      "INFO:tensorflow:loss = 0.1834183, step = 501 (18.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.46805\n",
      "INFO:tensorflow:loss = 0.14907482, step = 601 (18.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.44869\n",
      "INFO:tensorflow:loss = 0.15712944, step = 701 (18.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.46656\n",
      "INFO:tensorflow:loss = 0.102962166, step = 801 (18.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.31913\n",
      "INFO:tensorflow:loss = 0.1431313, step = 901 (18.799 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpkh1ba3b8\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.18672156.\n",
      "WARNING:tensorflow:From <ipython-input-38-be3c93f696ef>:26: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Yazhou\\AppData\\Local\\Temp\\tmpkh1ba3b8\\model.ckpt-1000\n",
      "Accuracy: 0.910850\n"
     ]
    }
   ],
   "source": [
    "def rnn_model(features, target):\n",
    "\t\"\"\"用RNN模型(这里用的是GRU)完成文本分类\"\"\"\n",
    "\t# Convert indexes of words into embeddings.\n",
    "\t# This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "\t# maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "\t# EMBEDDING_SIZE].\n",
    "    \"\"\n",
    "\tword_vectors = tf.contrib.layers.embed_sequence(\n",
    "\t\t\tfeatures, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\n",
    "\t# Split into list of embedding per word, while removing doc length dim.\n",
    "\t# word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "\tword_list = tf.unstack(word_vectors, axis=1)  # RNN 是将一个词一个词网里面灌，而不是像图片一样的两维\n",
    "\n",
    "\t# Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "\tcell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)  # 扔进来的词是20维的\n",
    "\n",
    "\t# Create an unrolled Recurrent Neural Networks to length of\n",
    "\t# MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "\t_, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)  # 利用GRU的cell 组成一个RNN\n",
    "\n",
    "\t# Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "\t# neural network of last step) and pass it as features for logistic\n",
    "\t# regression over output classes.\n",
    "\ttarget = tf.one_hot(target, 15, 1, 0)\n",
    "\tlogits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "\tloss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "\t# Create a training op.\n",
    "\ttrain_op = tf.contrib.layers.optimize_loss(\n",
    "\t\t\tloss,\n",
    "\t\t\ttf.contrib.framework.get_global_step(),\n",
    "\t\t\toptimizer='Adam',\n",
    "\t\t\tlearning_rate=0.01)\n",
    "\n",
    "\treturn ({\n",
    "\t\t\t'class': tf.argmax(logits, 1),\n",
    "\t\t\t'prob': tf.nn.softmax(logits)\n",
    "\t}, loss, train_op)\n",
    "\n",
    "model_fn = rnn_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train_ser, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test_ser, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
