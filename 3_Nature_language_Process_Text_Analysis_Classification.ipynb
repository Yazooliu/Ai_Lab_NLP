{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3_Nature_language_Process_Text_Analysis_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright private in 2018 \n",
    "#  Modify Date: \n",
    "#       2018 - 9 - 19\n",
    "#  Purpose : \n",
    "#       Text Analysise  by fasttext/word2vec/Deep learning/LSTM\n",
    "# ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext \n",
    "import pandas as pd \n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical softmax - 类别墅较多时，通过构建哈夫曼编码来技术softmax layer 计算 和之前的word2vec 的trick\n",
    "# N-gram - 之使用unigram 的话会丢掉word order 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当文本量大时，可以通过fasttext来学习\n",
    "# data Exampe: \n",
    "#__label__2,.......content ......\n",
    "#__label__3,......content......\n",
    "#__label__4,......content......\n",
    "\n",
    "# Data Category \n",
    "# 1. car 2.sports 3.entertainment 4. technology 5. military"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (一). 通过Facebook 工业界fasttext 模型根据输入的新闻内容预测该新闻所属的种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督学习 - 新闻分类/或者用于用户情感的褒贬分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 生成文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import random \n",
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"C:\\Python_\\technologynews.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"C:\\Python_\\carnews.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"C:\\Python_\\entertainmentnews.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sprots\n",
    "df_sprots   = pd.read_csv(\"C:\\Python_\\sportsnews.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"C:\\Python_\\military news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords  = pd.read_csv(\"data/stopwards.txt\", index_col = False, quoting = 3, sep = \"\\t\", names = ['stopward'], encoding = 'utf-8')\n",
    "stopwords  = stopwords['stopward'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Text Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\"__label__\"+str(category)+\", \"+\" \" .join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences,cate_dic['technology'])\n",
    "preprocess_text(car,sentences,cate_dic['car'])\n",
    "preprocess_text(entertainment,sentences,cate_dic['entertainment'])\n",
    "preprocess_text(military,sentences,cate_dic['military'])\n",
    "preprocess_text(sports,sentences,cate_dic['sports'])\n",
    "\n",
    "# 乱序处理 - 使得同一类别的样本不扎堆出现在一起\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.写入文本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"writing data to fasttext format\"\n",
    "openout  = open('training_datasets.txt','w')  # 写入的方式打开\n",
    "\n",
    "for sentence in sentences:\n",
    "    openout.write(sentence.encode('utf-8') + \"\\n\")  # 中文形式  encode('utf-8') + \"\\n\" 换行符\n",
    "print \"Done....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.调用fasttext 训练生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = fasttext.supervised('training_datasets.txt','classifier.model', label_prefix  = '__label__') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and evaluted the data set \n",
    "testresult = classifier.test('training_datasets.txt')\n",
    "\n",
    "# printing 准确值和召回率\n",
    "print 'testresult precision', testresult.precision \n",
    "print 'testresult recall', testresult.recall \n",
    "\n",
    "print 'Number of examples: ', testresult.nexamples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.实际预测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_cata_test = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "\n",
    "# 待测试样本\n",
    "texts  = ['马来西亚 反贪 委员会 称 马来西亚 前总理 纳吉布 被逮捕 因其牵涉']\n",
    "labels = classifier.predict(texts)\n",
    "\n",
    "#printing label and category \n",
    "print('labels is :', labels)\n",
    "print label_to_cata_test[int(labels[0][0])]\n",
    "\n",
    "\n",
    "# 同时输出有多少的概率来肯定种类是这个\n",
    "labels = classifier.predict_proba(texts)\n",
    "print labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TopK 预测结果分析\n",
    "# K = 5\n",
    "category = classifier.predict(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出category 及其对应的概率\n",
    "category = classifier.predict_proba(texts, K = 5)\n",
    "print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (二). 通过fasttext做无监督文本学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text(technology,sentences,cate_dic['technology'])\n",
    "preprocess_text(car,sentences,cate_dic['car'])\n",
    "preprocess_text(entertainment,sentences,cate_dic['entertainment'])\n",
    "preprocess_text(military,sentences,cate_dic['military'])\n",
    "preprocess_text(sports,sentences,cate_dic['sports'])\n",
    "\n",
    "\n",
    "# print out \n",
    "print \"writing data to  fasttext unsupervised learning format ...\"\n",
    "writeout = open('unsupervised_trainingdatasets.txt','w')\n",
    "\n",
    "for sentence in sentences: \n",
    "    writeout.write(sentence.encode('utf-8')+ \"\\n\")\n",
    "    \n",
    "print\"write Done ...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fasttext to training the data sets\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('unsupervised_trainingdatasets.txt','model')\n",
    "# print the list \n",
    "print model.words\n",
    "\n",
    "# CBOW model - continue bags of words \n",
    "model  = fasttext.cbow('unsupervised_trainingdatasets.txt','model')\n",
    "print model.words # list of words from dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (三). Gensim vs Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "def preprocess_text_unsupervised(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs))\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成无监督训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data - 无监督不需要标签\n",
    "preprocess_text(technology,sentences)\n",
    "preprocess_text(car,sentences)\n",
    "preprocess_text(entertainment,sentences)\n",
    "preprocess_text(military,sentences)\n",
    "preprocess_text(sports,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model fitting \n",
    "model = Word2Vec(sentences, size = 100, window = 5, min_count = 5, workers = 4)\n",
    "model.save(gensim_word2vec.model)\n",
    "model.wv['信息']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (四).文本分类by Deep Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: \n",
    "#并不是将全部数据全部加在到内容，而是将一个batch 一个batch 学习及权重更新去学习新的模型\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.CNN 做文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used on short text processing \n",
    "# LSTM can be used to long text processing\n",
    "\n",
    "# CNN 中的filter 窗口大小跟词向量的文本大小有关，每个词的窗口可能为 词向量的个数* 每个词向量的维数\n",
    "# 窗口filter + pooling 池化 + fullconnection 全链接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing and remove stopwards \n",
    "import pandas as pd \n",
    "\n",
    "# data set dict \n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"C:\\Python_\\technologynews.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna()  # 空的字符drop 掉\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"C:\\Python_\\carnews.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"C:\\Python_\\entertainmentnews.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_sprots\n",
    "df_sprots   = pd.read_csv(\"C:\\Python_\\sportsnews.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_military \n",
    "df_military  = pd.read_csv(\"C:\\Python_\\military news.csv\",encoding = 'utf-8')\n",
    "df_military  = df_military.dropna()\n",
    "\n",
    "\n",
    "# 提取出一定量的数据\n",
    "# .values -> array 数组\n",
    "# .tolist -> list 列表\n",
    "# [1000:21000] -> 切片找出一部分的数据\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car        = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[1000:21000]\n",
    "military   = df_military.content.values.tolist()[1000:21000]\n",
    "sports     = df_sports.content.values.tolist()[1000:21000]\n",
    "\n",
    "\n",
    "### remove stopwards\n",
    "stopwords = pd.read_csv(\"data/stopwards.txt\",index_col = False, quoting =3, sep= \"\\t\", names =['stopwards'],encoding = 'utf-8' )\n",
    "stopwords = stopwords['stopwords'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct Data \n",
    "# Data Preparation \n",
    "def preprocess_text_cnn(content_lines, sentences, category):\n",
    "    sentences = []\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x: len(x)>1,segs)  # len(x)<=1 为空，过滤掉\n",
    "            segs = filter(lambda x: x not in stopwards, segs)  # x in stopwards 过滤掉\n",
    "            # category 按照字典cate_dic 里面的value 取1/2/3/4 and so on \n",
    "            sentences.append(\" \".join(segs),category)  # 添加数据及其label\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue\n",
    "        \n",
    "\n",
    "## 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "# preprocess the data \n",
    "preprocess_text_cnn(technology,sentences,'technology')\n",
    "preprocess_text_cnn(car,sentences,'car')\n",
    "preprocess_text_cnn(entertainment,sentences,'entertainment')\n",
    "preprocess_text_cnn(military,sentences,'military')\n",
    "preprocess_text_cnn(sports,sentences,'sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data sets \n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "# 拉链 将词语和label 分别分给x and y \n",
    "x,y = zip(*sentences)\n",
    "\n",
    "# split the data into trianing and test data sets \n",
    "train_data,test_data,train_target,test_target  = train_test_split(x,y,random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 构建神经网络过程 - 中文文本分类 on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import np\n",
    "from sklrearn import metrics \n",
    "import tensorflow as tf \n",
    "\n",
    "learn  = tf.contrib.learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables Initialization\n",
    "\n",
    "FLAGS = None\n",
    "# 文档最长的长度\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "# 最小词频数\n",
    "MIN_WORD_FREQUENCY = 2 \n",
    "\n",
    "# 词嵌入的维度\n",
    "EMBEDDING_SIZE= 20\n",
    "\n",
    "# filter 数量\n",
    "N_FILTERS = 10\n",
    "\n",
    "# Windows size \n",
    "WINDOWS_SIZE = 20\n",
    "\n",
    "#filter 的形状\n",
    "FILTER_SHAPE1 = [WINDOWS_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOWS_SIZE, N_FILTERS]\n",
    "\n",
    "# Pooling \n",
    "POOLING_WINDOW  = 4\n",
    "POOLING_STRIDE  = 2 \n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN Model 卷积神经网络\n",
    "def cnn_model(features,target):\n",
    "    ###\n",
    "    ### 两层的卷积神经网络，用于短文本分类\n",
    "    # 先把词转成词嵌入\n",
    "    # 我们得到一个形状为[n_words,EMBEDDING_SIZE] 的词表映射矩阵\n",
    "    # 接着我们可以把一批文本映射成[batch_size,sequence_length,EMBEDDING_SIZE]的矩阵\n",
    "    \n",
    "    # one - hot 编码 \n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    \n",
    "    # 将feature/文本 的序列做一个映射，编成一个二维向量\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(features, vocab_size = n_words, embed_dim = EMBEDDING_SIZE,scope = 'words')\n",
    "    \n",
    "    # 将2维转成3 维\n",
    "    word_vectors = tf.expand_dims(word_vectors,3)\n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        # 添加一个二维的卷积滤波\n",
    "        conv1  = tf.contrib.layers.convolution2d(word_vectors,N_FILTERS,FILTER_SHAPE1,padding = 'VALID')\n",
    "        # 添加RELU非线性 - 激活函数\n",
    "        conv1  = tf.nn.relu(conv1)\n",
    "        \n",
    "        # maxmimum pooling \n",
    "        pool1 = tf.nn.max_pool(conv1,ksize = [1,POOLING_WINDOW,1,1], strides = [1,POOLING_STRIDE,1,1], padding = 'SAME')\n",
    "        \n",
    "        # 对矩阵转置 以满足形状\n",
    "        pool1 = tf.transpose(pool1,[0,1,3,2])\n",
    "        \n",
    "    \n",
    "    with tf.variabel_scope('CNN_Layer2'):\n",
    "            # 第2个卷积层\n",
    "            conv2 = tf.contrib.layers.convolution2d(pool1,N_FILTERS,FILTER_SHAPE2,padding = 'VALID')\n",
    "            \n",
    "            # 抽取特征\n",
    "            pool2  = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims = [1])\n",
    "            \n",
    "            \n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    logits = tf.contrib.layer.fully_connected(pool2, 15, activation_fn = None)  # 无激活函数\n",
    "    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framwork.get_gloabl_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "    \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tensorflow.preprocessing 里的VocabularyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp  = ['I am good', 'You are  here ','I am glad', 'it is great']\n",
    "#\n",
    "# 只要出现的最小频率是1 或者比1 大，就处理\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(10, min_frequency=1)\n",
    "list(vocab_processor.fit_transform(temp))\n",
    "\n",
    "# I am good -> [1,2,0,0,......] length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data)))\n",
    "x_test = np.array(list(vocab_processor.fit_transform(test_data)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d', %n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "# 将类别映射成数字\n",
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "train_target = map(lambda x:cate_dic[x],train_target )\n",
    "test_target = map(lambda x:cate_dic[x],test_target )\n",
    "\n",
    "y_train =  pandas.Series(train_target)\n",
    "y_test  =  pandas.Series(train_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn = cnn_model))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(x_train,y_train,steps = 1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test,y_predicted)\n",
    "print('Accuracy:{0:f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 LSTM/GRU - RNN 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用RNN完成文本分类\n",
    "# for python2 need to import lib\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function \n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd \n",
    "import np\n",
    "from sklrearn import metrics \n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import encoders \n",
    "learn  = tf.contrib,learn\n",
    "\n",
    "FLAGS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过词袋模型来一批一批的把数据灌进去\n",
    "MAX_DOCUMENT_LENGTN = 15\n",
    "MIN_WORD_FREQUENCE  = 1\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor  = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency = 1)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data)))\n",
    "x_test = np.array(list(vocab_processor.fit_transform(test_data)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d', %n_words)\n",
    "\n",
    "def bag_of_words_model(feature,target):\n",
    "    # 生成词袋模型\n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    features = encoders.bow_encoder(features,vocab_size = n_words,embed_dim = EMBEDDING_SIZE)\n",
    "\n",
    "    # FullConnection - 全链接\n",
    "    # 预测值 : logits\n",
    "    logits = tf.contrib.layer.fully_connected(pool2, 15, activation_fn = None)  # 无激活函数\n",
    "    loss   = tf.losses.softmax_cross_entropy(target, logits)  #target:真实值， logits；预测值\n",
    "    \n",
    "    \n",
    "    # 循环迭代\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss,tf.contrib.framwork.get_gloabl_step(), optimizer = 'Adam', learning_rate = 0.01)\n",
    "  \n",
    "    # return\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n",
    "\n",
    "#\n",
    "model_fn = bag_of_words_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn = model_fn))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(x_train,y_train,steps = 1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "\n",
    "score = metrics.accuracy_score(y_test,y_predicted)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
